{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6010308570287474766\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 441581568\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9706691044120419219\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0, compute capability: 3.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gym\n",
    "import time\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Activation, Flatten, BatchNormalization, Conv2D, Add\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import History\n",
    "from keras.backend import tf as ktf\n",
    "from keras.callbacks import Callback as KerasCallback, CallbackList as KerasCallbackList\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, CSVLogger\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras import backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy, Policy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import ModelIntervalCheckpoint, FileLogger\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# check our devices\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from gym import error, spaces\n",
    "\n",
    "class Env(object):\n",
    "    \"\"\"The abstract environment class that is used by all agents. This class has the exact\n",
    "    same API that OpenAI Gym uses so that integrating with it is trivial. In contrast to the\n",
    "    OpenAI Gym implementation, this class only defines the abstract methods without any actual\n",
    "    implementation.\n",
    "    To implement your own environment, you need to define the following methods:\n",
    "    - `step`\n",
    "    - `reset`\n",
    "    - `render`\n",
    "    - `close`\n",
    "    Refer to the [Gym documentation](https://gym.openai.com/docs/#environments).\n",
    "    \"\"\"\n",
    "    reward_range = (-np.inf, np.inf)\n",
    "    action_space = spaces.Discrete(4)\n",
    "    observation_space = spaces.Box(low=-1, high=1, shape=(3,3))\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_num += 1\n",
    "        done = False\n",
    "        previous_distance = abs(self.pos[0] - 2) + abs(self.pos[1] - 2)\n",
    "        \n",
    "        if action == 0:\n",
    "            # up\n",
    "            self.pos = (max(self.pos[0] - 1, 0), self.pos[1])\n",
    "        if action == 1:\n",
    "            # down\n",
    "            self.pos = (min(self.pos[0] + 1, 2), self.pos[1])\n",
    "        if action == 2:\n",
    "            # left\n",
    "            self.pos = (self.pos[0], max(self.pos[1] - 1, 0))\n",
    "        if action == 3:\n",
    "            # right\n",
    "            self.pos = (self.pos[0], min(self.pos[1] + 1, 2))\n",
    "            \n",
    "        current_distance = abs(self.pos[0] - 2) + abs(self.pos[1] - 2)\n",
    "        \n",
    "        reward = -0.1\n",
    "        \n",
    "        if previous_distance < current_distance:\n",
    "            reward = -1.\n",
    "            \n",
    "        if previous_distance > current_distance:\n",
    "            reward = 1.\n",
    "            \n",
    "        if current_distance == 0:\n",
    "            done = True\n",
    "        \n",
    "        self.observation = self.default_view.copy()\n",
    "        self.observation[self.pos] = 0.5\n",
    "        \n",
    "        if self.step_num > 10:\n",
    "            done= True\n",
    "        \n",
    "        return self.observation, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_num = 0\n",
    "        self.pos = (0,0)\n",
    "        self.default_view = np.array([\n",
    "            [0., 0., 0.],\n",
    "            [0., 0., 0.],\n",
    "            [0., 0., 1.]\n",
    "        ])\n",
    "        self.observation = self.default_view.copy()\n",
    "        self.observation[self.pos] = 0.5\n",
    "        return self.observation\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        plt.imshow(self.observation)\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Override in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        # Returns\n",
    "            Returns the list of seeds used in this env's random number generators\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        \"\"\"Provides runtime configuration to the environment.\n",
    "        This configuration should consist of data that tells your\n",
    "        environment how to run (such as an address of a remote server,\n",
    "        or path to your ImageNet data). It should not affect the\n",
    "        semantics of the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "input_shape = (1, 3, 3)\n",
    "\n",
    "frames_input = keras.layers.Input(input_shape, name='frames')\n",
    "\n",
    "flat = keras.layers.Flatten()(frames_input)\n",
    "hidden_1 = keras.layers.Dense(512, activation='relu')(flat)\n",
    "hidden_2 = keras.layers.Dense(512, activation='relu')(hidden_1)\n",
    "output = keras.layers.Dense(nb_actions)(hidden_2)\n",
    "\n",
    "model = keras.models.Model(inputs=frames_input, outputs=output)\n",
    "optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "\n",
    "memory = SequentialMemory(limit=1000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), 'eps', 1., 0., 0.0, 1000)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=500, \n",
    "               target_model_update=1000, policy=policy, test_policy=policy)\n",
    "dqn.compile(optimizer, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "   11/5000: episode: 1, duration: 0.702s, episode steps: 11, steps per second: 16, episode reward: 0.800, mean reward: 0.073 [-1.000, 1.000], mean action: 1.727 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "   18/5000: episode: 2, duration: 0.022s, episode steps: 7, steps per second: 320, episode reward: 3.900, mean reward: 0.557 [-1.000, 1.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.151 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "   29/5000: episode: 3, duration: 0.033s, episode steps: 11, steps per second: 332, episode reward: 0.600, mean reward: 0.055 [-1.000, 1.000], mean action: 0.909 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "   40/5000: episode: 4, duration: 0.034s, episode steps: 11, steps per second: 321, episode reward: 1.900, mean reward: 0.173 [-1.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "   51/5000: episode: 5, duration: 0.037s, episode steps: 11, steps per second: 297, episode reward: -0.300, mean reward: -0.027 [-1.000, 1.000], mean action: 1.636 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "   62/5000: episode: 6, duration: 0.031s, episode steps: 11, steps per second: 355, episode reward: 3.900, mean reward: 0.355 [-1.000, 1.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.157 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "   73/5000: episode: 7, duration: 0.035s, episode steps: 11, steps per second: 314, episode reward: 2.400, mean reward: 0.218 [-1.000, 1.000], mean action: 1.727 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "   84/5000: episode: 8, duration: 0.036s, episode steps: 11, steps per second: 310, episode reward: 0.800, mean reward: 0.073 [-1.000, 1.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "   95/5000: episode: 9, duration: 0.035s, episode steps: 11, steps per second: 318, episode reward: 1.700, mean reward: 0.155 [-1.000, 1.000], mean action: 1.545 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "   99/5000: episode: 10, duration: 0.014s, episode steps: 4, steps per second: 292, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  106/5000: episode: 11, duration: 0.022s, episode steps: 7, steps per second: 311, episode reward: 3.900, mean reward: 0.557 [-1.000, 1.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.151 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  117/5000: episode: 12, duration: 0.038s, episode steps: 11, steps per second: 293, episode reward: -0.300, mean reward: -0.027 [-1.000, 1.000], mean action: 1.545 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  128/5000: episode: 13, duration: 0.037s, episode steps: 11, steps per second: 297, episode reward: 2.600, mean reward: 0.236 [-1.000, 1.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  139/5000: episode: 14, duration: 0.034s, episode steps: 11, steps per second: 328, episode reward: -0.500, mean reward: -0.045 [-1.000, 1.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  150/5000: episode: 15, duration: 0.035s, episode steps: 11, steps per second: 315, episode reward: 0.400, mean reward: 0.036 [-1.000, 1.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  161/5000: episode: 16, duration: 0.035s, episode steps: 11, steps per second: 313, episode reward: 1.700, mean reward: 0.155 [-1.000, 1.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  172/5000: episode: 17, duration: 0.032s, episode steps: 11, steps per second: 347, episode reward: 1.700, mean reward: 0.155 [-1.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  183/5000: episode: 18, duration: 0.033s, episode steps: 11, steps per second: 335, episode reward: 0.600, mean reward: 0.055 [-1.000, 1.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  194/5000: episode: 19, duration: 0.035s, episode steps: 11, steps per second: 318, episode reward: 0.200, mean reward: 0.018 [-1.000, 1.000], mean action: 1.091 [0.000, 2.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  205/5000: episode: 20, duration: 0.034s, episode steps: 11, steps per second: 324, episode reward: -0.000, mean reward: -0.000 [-0.100, 1.000], mean action: 0.818 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  216/5000: episode: 21, duration: 0.022s, episode steps: 11, steps per second: 504, episode reward: 2.800, mean reward: 0.255 [-1.000, 1.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  227/5000: episode: 22, duration: 0.020s, episode steps: 11, steps per second: 552, episode reward: 1.500, mean reward: 0.136 [-1.000, 1.000], mean action: 1.545 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  238/5000: episode: 23, duration: 0.015s, episode steps: 11, steps per second: 733, episode reward: 2.800, mean reward: 0.255 [-1.000, 1.000], mean action: 1.636 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  249/5000: episode: 24, duration: 0.019s, episode steps: 11, steps per second: 588, episode reward: 0.600, mean reward: 0.055 [-1.000, 1.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  255/5000: episode: 25, duration: 0.011s, episode steps: 6, steps per second: 561, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 1.833 [0.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  266/5000: episode: 26, duration: 0.015s, episode steps: 11, steps per second: 749, episode reward: 3.300, mean reward: 0.300 [-0.100, 1.000], mean action: 0.909 [0.000, 3.000], mean observation: 0.157 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  277/5000: episode: 27, duration: 0.020s, episode steps: 11, steps per second: 545, episode reward: 1.500, mean reward: 0.136 [-1.000, 1.000], mean action: 0.909 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  288/5000: episode: 28, duration: 0.020s, episode steps: 11, steps per second: 548, episode reward: 1.300, mean reward: 0.118 [-1.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  299/5000: episode: 29, duration: 0.016s, episode steps: 11, steps per second: 692, episode reward: 1.000, mean reward: 0.091 [-1.000, 1.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  310/5000: episode: 30, duration: 0.018s, episode steps: 11, steps per second: 612, episode reward: 1.700, mean reward: 0.155 [-1.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  321/5000: episode: 31, duration: 0.017s, episode steps: 11, steps per second: 629, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 0.818 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  332/5000: episode: 32, duration: 0.016s, episode steps: 11, steps per second: 672, episode reward: 1.500, mean reward: 0.136 [-1.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  343/5000: episode: 33, duration: 0.018s, episode steps: 11, steps per second: 607, episode reward: 1.500, mean reward: 0.136 [-1.000, 1.000], mean action: 0.818 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  354/5000: episode: 34, duration: 0.024s, episode steps: 11, steps per second: 462, episode reward: 3.300, mean reward: 0.300 [-0.100, 1.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.157 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  365/5000: episode: 35, duration: 0.019s, episode steps: 11, steps per second: 564, episode reward: -0.700, mean reward: -0.064 [-1.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  376/5000: episode: 36, duration: 0.019s, episode steps: 11, steps per second: 593, episode reward: 1.300, mean reward: 0.118 [-1.000, 1.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  387/5000: episode: 37, duration: 0.017s, episode steps: 11, steps per second: 648, episode reward: 2.800, mean reward: 0.255 [-1.000, 1.000], mean action: 0.909 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  398/5000: episode: 38, duration: 0.016s, episode steps: 11, steps per second: 691, episode reward: 2.600, mean reward: 0.236 [-1.000, 1.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  409/5000: episode: 39, duration: 0.017s, episode steps: 11, steps per second: 645, episode reward: 1.700, mean reward: 0.155 [-1.000, 1.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  420/5000: episode: 40, duration: 0.017s, episode steps: 11, steps per second: 661, episode reward: -0.700, mean reward: -0.064 [-1.000, 1.000], mean action: 0.909 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  431/5000: episode: 41, duration: 0.016s, episode steps: 11, steps per second: 670, episode reward: 1.300, mean reward: 0.118 [-1.000, 1.000], mean action: 0.636 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  442/5000: episode: 42, duration: 0.022s, episode steps: 11, steps per second: 493, episode reward: 0.400, mean reward: 0.036 [-1.000, 1.000], mean action: 0.818 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  453/5000: episode: 43, duration: 0.019s, episode steps: 11, steps per second: 590, episode reward: 1.700, mean reward: 0.155 [-1.000, 1.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  464/5000: episode: 44, duration: 0.016s, episode steps: 11, steps per second: 671, episode reward: 1.700, mean reward: 0.155 [-1.000, 1.000], mean action: 1.182 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  473/5000: episode: 45, duration: 0.012s, episode steps: 9, steps per second: 769, episode reward: 3.500, mean reward: 0.389 [-0.100, 1.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.154 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  484/5000: episode: 46, duration: 0.018s, episode steps: 11, steps per second: 628, episode reward: 1.900, mean reward: 0.173 [-1.000, 1.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  495/5000: episode: 47, duration: 0.014s, episode steps: 11, steps per second: 781, episode reward: -0.900, mean reward: -0.082 [-1.000, 1.000], mean action: 0.636 [0.000, 2.000], mean observation: 0.167 [0.000, 1.000], loss: --, mean_squared_error: --, mean_q: --, mean_eps: --\n",
      "  506/5000: episode: 48, duration: 1.358s, episode steps: 11, steps per second: 8, episode reward: 1.500, mean reward: 0.136 [-1.000, 1.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.252712, mean_squared_error: 0.127731, mean_q: 0.051827, mean_eps: 0.497000\n",
      "  511/5000: episode: 49, duration: 0.070s, episode steps: 5, steps per second: 72, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 2.200 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.234683, mean_squared_error: 0.122503, mean_q: 0.099327, mean_eps: 0.492000\n",
      "  522/5000: episode: 50, duration: 0.153s, episode steps: 11, steps per second: 72, episode reward: -0.500, mean reward: -0.045 [-1.000, 1.000], mean action: 1.818 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.234777, mean_squared_error: 0.133147, mean_q: 0.178049, mean_eps: 0.484000\n",
      "  530/5000: episode: 51, duration: 0.114s, episode steps: 8, steps per second: 70, episode reward: 3.800, mean reward: 0.475 [-1.000, 1.000], mean action: 1.875 [0.000, 3.000], mean observation: 0.153 [0.000, 1.000], loss: 0.189802, mean_squared_error: 0.129049, mean_q: 0.257049, mean_eps: 0.474500\n",
      "  537/5000: episode: 52, duration: 0.098s, episode steps: 7, steps per second: 72, episode reward: 3.900, mean reward: 0.557 [-1.000, 1.000], mean action: 2.143 [1.000, 3.000], mean observation: 0.151 [0.000, 1.000], loss: 0.174237, mean_squared_error: 0.135746, mean_q: 0.304777, mean_eps: 0.467000\n",
      "  547/5000: episode: 53, duration: 0.129s, episode steps: 10, steps per second: 78, episode reward: 3.800, mean reward: 0.380 [-1.000, 1.000], mean action: 2.300 [1.000, 3.000], mean observation: 0.156 [0.000, 1.000], loss: 0.154908, mean_squared_error: 0.147405, mean_q: 0.371879, mean_eps: 0.458500\n",
      "  558/5000: episode: 54, duration: 0.144s, episode steps: 11, steps per second: 76, episode reward: -0.300, mean reward: -0.027 [-1.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.134053, mean_squared_error: 0.166880, mean_q: 0.445721, mean_eps: 0.448000\n",
      "  562/5000: episode: 55, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.119532, mean_squared_error: 0.183561, mean_q: 0.501581, mean_eps: 0.440500\n",
      "  566/5000: episode: 56, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.108153, mean_squared_error: 0.196879, mean_q: 0.538050, mean_eps: 0.436500\n",
      "  570/5000: episode: 57, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.098277, mean_squared_error: 0.210774, mean_q: 0.561133, mean_eps: 0.432500\n",
      "  576/5000: episode: 58, duration: 0.083s, episode steps: 6, steps per second: 72, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 2.167 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.093397, mean_squared_error: 0.226952, mean_q: 0.598028, mean_eps: 0.427500\n",
      "  583/5000: episode: 59, duration: 0.100s, episode steps: 7, steps per second: 70, episode reward: 3.900, mean reward: 0.557 [-1.000, 1.000], mean action: 1.857 [0.000, 3.000], mean observation: 0.151 [0.000, 1.000], loss: 0.092864, mean_squared_error: 0.242474, mean_q: 0.627979, mean_eps: 0.421000\n",
      "  591/5000: episode: 60, duration: 0.114s, episode steps: 8, steps per second: 70, episode reward: 3.800, mean reward: 0.475 [-1.000, 1.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.153 [0.000, 1.000], loss: 0.083958, mean_squared_error: 0.270249, mean_q: 0.676720, mean_eps: 0.413500\n",
      "  596/5000: episode: 61, duration: 0.072s, episode steps: 5, steps per second: 70, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.600 [0.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.069638, mean_squared_error: 0.280491, mean_q: 0.711765, mean_eps: 0.407000\n",
      "  607/5000: episode: 62, duration: 0.140s, episode steps: 11, steps per second: 78, episode reward: 3.700, mean reward: 0.336 [-1.000, 1.000], mean action: 1.909 [1.000, 3.000], mean observation: 0.157 [0.000, 1.000], loss: 0.067841, mean_squared_error: 0.307019, mean_q: 0.752222, mean_eps: 0.399000\n",
      "  617/5000: episode: 63, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 3.600, mean reward: 0.360 [-1.000, 1.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.156 [0.000, 1.000], loss: 0.064140, mean_squared_error: 0.336826, mean_q: 0.808862, mean_eps: 0.388500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  628/5000: episode: 64, duration: 0.135s, episode steps: 11, steps per second: 81, episode reward: 2.600, mean reward: 0.236 [-1.000, 1.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.063595, mean_squared_error: 0.358716, mean_q: 0.843621, mean_eps: 0.378000\n",
      "  632/5000: episode: 65, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.072064, mean_squared_error: 0.367738, mean_q: 0.852170, mean_eps: 0.370500\n",
      "  637/5000: episode: 66, duration: 0.067s, episode steps: 5, steps per second: 74, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.064179, mean_squared_error: 0.380046, mean_q: 0.863637, mean_eps: 0.366000\n",
      "  648/5000: episode: 67, duration: 0.139s, episode steps: 11, steps per second: 79, episode reward: 1.300, mean reward: 0.118 [-1.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.072775, mean_squared_error: 0.384876, mean_q: 0.865141, mean_eps: 0.358000\n",
      "  659/5000: episode: 68, duration: 0.135s, episode steps: 11, steps per second: 81, episode reward: 2.400, mean reward: 0.218 [-1.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.069205, mean_squared_error: 0.380547, mean_q: 0.861117, mean_eps: 0.347000\n",
      "  665/5000: episode: 69, duration: 0.085s, episode steps: 6, steps per second: 71, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.064431, mean_squared_error: 0.386661, mean_q: 0.863897, mean_eps: 0.338500\n",
      "  670/5000: episode: 70, duration: 0.068s, episode steps: 5, steps per second: 74, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.600 [0.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.061871, mean_squared_error: 0.385487, mean_q: 0.864969, mean_eps: 0.333000\n",
      "  674/5000: episode: 71, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.069156, mean_squared_error: 0.386433, mean_q: 0.867726, mean_eps: 0.328500\n",
      "  684/5000: episode: 72, duration: 0.134s, episode steps: 10, steps per second: 74, episode reward: 3.800, mean reward: 0.380 [-1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.156 [0.000, 1.000], loss: 0.068665, mean_squared_error: 0.381543, mean_q: 0.861434, mean_eps: 0.321500\n",
      "  689/5000: episode: 73, duration: 0.068s, episode steps: 5, steps per second: 74, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.600 [0.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.062335, mean_squared_error: 0.388151, mean_q: 0.864816, mean_eps: 0.314000\n",
      "  695/5000: episode: 74, duration: 0.080s, episode steps: 6, steps per second: 75, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 1.667 [0.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.066114, mean_squared_error: 0.394871, mean_q: 0.874651, mean_eps: 0.308500\n",
      "  701/5000: episode: 75, duration: 0.093s, episode steps: 6, steps per second: 64, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 2.167 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.059590, mean_squared_error: 0.391609, mean_q: 0.877555, mean_eps: 0.302500\n",
      "  709/5000: episode: 76, duration: 0.112s, episode steps: 8, steps per second: 71, episode reward: 4.000, mean reward: 0.500 [-1.000, 1.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.153 [0.000, 1.000], loss: 0.075957, mean_squared_error: 0.400484, mean_q: 0.873122, mean_eps: 0.295500\n",
      "  715/5000: episode: 77, duration: 0.084s, episode steps: 6, steps per second: 71, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 2.167 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.065096, mean_squared_error: 0.390311, mean_q: 0.868852, mean_eps: 0.288500\n",
      "  720/5000: episode: 78, duration: 0.076s, episode steps: 5, steps per second: 66, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.066720, mean_squared_error: 0.394257, mean_q: 0.874499, mean_eps: 0.283000\n",
      "  731/5000: episode: 79, duration: 0.144s, episode steps: 11, steps per second: 77, episode reward: 2.800, mean reward: 0.255 [-1.000, 1.000], mean action: 2.091 [0.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.059035, mean_squared_error: 0.386964, mean_q: 0.878593, mean_eps: 0.275000\n",
      "  738/5000: episode: 80, duration: 0.092s, episode steps: 7, steps per second: 76, episode reward: 3.900, mean reward: 0.557 [-1.000, 1.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.151 [0.000, 1.000], loss: 0.062017, mean_squared_error: 0.397174, mean_q: 0.880180, mean_eps: 0.266000\n",
      "  742/5000: episode: 81, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.051890, mean_squared_error: 0.389379, mean_q: 0.889046, mean_eps: 0.260500\n",
      "  746/5000: episode: 82, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.055569, mean_squared_error: 0.401297, mean_q: 0.888895, mean_eps: 0.256500\n",
      "  751/5000: episode: 83, duration: 0.071s, episode steps: 5, steps per second: 70, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.600 [0.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.060689, mean_squared_error: 0.393725, mean_q: 0.896184, mean_eps: 0.252000\n",
      "  757/5000: episode: 84, duration: 0.084s, episode steps: 6, steps per second: 72, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.061917, mean_squared_error: 0.404179, mean_q: 0.896815, mean_eps: 0.246500\n",
      "  762/5000: episode: 85, duration: 0.069s, episode steps: 5, steps per second: 73, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.055371, mean_squared_error: 0.396392, mean_q: 0.886568, mean_eps: 0.241000\n",
      "  768/5000: episode: 86, duration: 0.077s, episode steps: 6, steps per second: 78, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 2.167 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.053087, mean_squared_error: 0.394322, mean_q: 0.891641, mean_eps: 0.235500\n",
      "  772/5000: episode: 87, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.050508, mean_squared_error: 0.391066, mean_q: 0.902616, mean_eps: 0.230500\n",
      "  776/5000: episode: 88, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.048293, mean_squared_error: 0.396288, mean_q: 0.894966, mean_eps: 0.226500\n",
      "  782/5000: episode: 89, duration: 0.080s, episode steps: 6, steps per second: 75, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.057116, mean_squared_error: 0.407873, mean_q: 0.901173, mean_eps: 0.221500\n",
      "  788/5000: episode: 90, duration: 0.079s, episode steps: 6, steps per second: 76, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.062174, mean_squared_error: 0.412415, mean_q: 0.893633, mean_eps: 0.215500\n",
      "  792/5000: episode: 91, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.057259, mean_squared_error: 0.402743, mean_q: 0.899196, mean_eps: 0.210500\n",
      "  796/5000: episode: 92, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.063046, mean_squared_error: 0.408204, mean_q: 0.896766, mean_eps: 0.206500\n",
      "  800/5000: episode: 93, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.068184, mean_squared_error: 0.415171, mean_q: 0.894793, mean_eps: 0.202500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  804/5000: episode: 94, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.052600, mean_squared_error: 0.386575, mean_q: 0.899844, mean_eps: 0.198500\n",
      "  808/5000: episode: 95, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.068539, mean_squared_error: 0.404069, mean_q: 0.889691, mean_eps: 0.194500\n",
      "  812/5000: episode: 96, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.046819, mean_squared_error: 0.400833, mean_q: 0.905375, mean_eps: 0.190500\n",
      "  818/5000: episode: 97, duration: 0.082s, episode steps: 6, steps per second: 73, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.050293, mean_squared_error: 0.410637, mean_q: 0.912679, mean_eps: 0.185500\n",
      "  822/5000: episode: 98, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.046501, mean_squared_error: 0.414524, mean_q: 0.916838, mean_eps: 0.180500\n",
      "  826/5000: episode: 99, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.054761, mean_squared_error: 0.419515, mean_q: 0.911809, mean_eps: 0.176500\n",
      "  830/5000: episode: 100, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.049259, mean_squared_error: 0.398762, mean_q: 0.913549, mean_eps: 0.172500\n",
      "  835/5000: episode: 101, duration: 0.068s, episode steps: 5, steps per second: 73, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 2.200 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.043554, mean_squared_error: 0.404411, mean_q: 0.916408, mean_eps: 0.168000\n",
      "  841/5000: episode: 102, duration: 0.078s, episode steps: 6, steps per second: 77, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.050157, mean_squared_error: 0.393403, mean_q: 0.918761, mean_eps: 0.162500\n",
      "  845/5000: episode: 103, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.044344, mean_squared_error: 0.395458, mean_q: 0.916911, mean_eps: 0.157500\n",
      "  849/5000: episode: 104, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.052663, mean_squared_error: 0.404937, mean_q: 0.920486, mean_eps: 0.153500\n",
      "  853/5000: episode: 105, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.044374, mean_squared_error: 0.402039, mean_q: 0.931293, mean_eps: 0.149500\n",
      "  857/5000: episode: 106, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.036606, mean_squared_error: 0.411671, mean_q: 0.940545, mean_eps: 0.145500\n",
      "  861/5000: episode: 107, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.035516, mean_squared_error: 0.395496, mean_q: 0.930333, mean_eps: 0.141500\n",
      "  865/5000: episode: 108, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.033827, mean_squared_error: 0.399485, mean_q: 0.944517, mean_eps: 0.137500\n",
      "  869/5000: episode: 109, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.065633, mean_squared_error: 0.421096, mean_q: 0.923631, mean_eps: 0.133500\n",
      "  873/5000: episode: 110, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.034546, mean_squared_error: 0.401585, mean_q: 0.933344, mean_eps: 0.129500\n",
      "  878/5000: episode: 111, duration: 0.077s, episode steps: 5, steps per second: 65, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.041145, mean_squared_error: 0.417272, mean_q: 0.942896, mean_eps: 0.125000\n",
      "  884/5000: episode: 112, duration: 0.083s, episode steps: 6, steps per second: 72, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.049423, mean_squared_error: 0.403944, mean_q: 0.937353, mean_eps: 0.119500\n",
      "  888/5000: episode: 113, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.045849, mean_squared_error: 0.402156, mean_q: 0.920321, mean_eps: 0.114500\n",
      "  892/5000: episode: 114, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.041353, mean_squared_error: 0.412429, mean_q: 0.929928, mean_eps: 0.110500\n",
      "  896/5000: episode: 115, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.039150, mean_squared_error: 0.414660, mean_q: 0.933320, mean_eps: 0.106500\n",
      "  902/5000: episode: 116, duration: 0.083s, episode steps: 6, steps per second: 73, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.056996, mean_squared_error: 0.400020, mean_q: 0.928747, mean_eps: 0.101500\n",
      "  906/5000: episode: 117, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.043513, mean_squared_error: 0.382512, mean_q: 0.928491, mean_eps: 0.096500\n",
      "  910/5000: episode: 118, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.036878, mean_squared_error: 0.395691, mean_q: 0.934022, mean_eps: 0.092500\n",
      "  914/5000: episode: 119, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.042041, mean_squared_error: 0.407847, mean_q: 0.941391, mean_eps: 0.088500\n",
      "  920/5000: episode: 120, duration: 0.079s, episode steps: 6, steps per second: 76, episode reward: 4.000, mean reward: 0.667 [-1.000, 1.000], mean action: 2.167 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.047750, mean_squared_error: 0.408891, mean_q: 0.934697, mean_eps: 0.083500\n",
      "  924/5000: episode: 121, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.051531, mean_squared_error: 0.402122, mean_q: 0.934298, mean_eps: 0.078500\n",
      "  928/5000: episode: 122, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.039930, mean_squared_error: 0.387393, mean_q: 0.925880, mean_eps: 0.074500\n",
      "  932/5000: episode: 123, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.047586, mean_squared_error: 0.387540, mean_q: 0.920606, mean_eps: 0.070500\n",
      "  936/5000: episode: 124, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.031219, mean_squared_error: 0.387789, mean_q: 0.930490, mean_eps: 0.066500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  940/5000: episode: 125, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.048300, mean_squared_error: 0.392013, mean_q: 0.933800, mean_eps: 0.062500\n",
      "  944/5000: episode: 126, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.029658, mean_squared_error: 0.399319, mean_q: 0.939269, mean_eps: 0.058500\n",
      "  948/5000: episode: 127, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.034120, mean_squared_error: 0.391583, mean_q: 0.946710, mean_eps: 0.054500\n",
      "  952/5000: episode: 128, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.033388, mean_squared_error: 0.391752, mean_q: 0.952021, mean_eps: 0.050500\n",
      "  957/5000: episode: 129, duration: 0.071s, episode steps: 5, steps per second: 71, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 2.200 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.031175, mean_squared_error: 0.399841, mean_q: 0.950121, mean_eps: 0.046000\n",
      "  961/5000: episode: 130, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.033071, mean_squared_error: 0.395598, mean_q: 0.942813, mean_eps: 0.041500\n",
      "  965/5000: episode: 131, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.038890, mean_squared_error: 0.404262, mean_q: 0.937967, mean_eps: 0.037500\n",
      "  969/5000: episode: 132, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.039691, mean_squared_error: 0.412937, mean_q: 0.950537, mean_eps: 0.033500\n",
      "  973/5000: episode: 133, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.048755, mean_squared_error: 0.411659, mean_q: 0.941405, mean_eps: 0.029500\n",
      "  977/5000: episode: 134, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.030142, mean_squared_error: 0.404157, mean_q: 0.953714, mean_eps: 0.025500\n",
      "  981/5000: episode: 135, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.031016, mean_squared_error: 0.390914, mean_q: 0.938570, mean_eps: 0.021500\n",
      "  985/5000: episode: 136, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.031430, mean_squared_error: 0.392798, mean_q: 0.945031, mean_eps: 0.017500\n",
      "  989/5000: episode: 137, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.041773, mean_squared_error: 0.411819, mean_q: 0.945228, mean_eps: 0.013500\n",
      "  993/5000: episode: 138, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.036050, mean_squared_error: 0.393899, mean_q: 0.947950, mean_eps: 0.009500\n",
      "  997/5000: episode: 139, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.028946, mean_squared_error: 0.386544, mean_q: 0.954931, mean_eps: 0.005500\n",
      " 1001/5000: episode: 140, duration: 0.065s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.023775, mean_squared_error: 0.392475, mean_q: 0.949170, mean_eps: 0.001500\n",
      " 1005/5000: episode: 141, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.342739, mean_squared_error: 0.591075, mean_q: 1.020705, mean_eps: 0.000000\n",
      " 1009/5000: episode: 142, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.300361, mean_squared_error: 0.629726, mean_q: 1.138952, mean_eps: 0.000000\n",
      " 1013/5000: episode: 143, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.230568, mean_squared_error: 0.649972, mean_q: 1.235364, mean_eps: 0.000000\n",
      " 1017/5000: episode: 144, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.197423, mean_squared_error: 0.677734, mean_q: 1.300634, mean_eps: 0.000000\n",
      " 1021/5000: episode: 145, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.168332, mean_squared_error: 0.719457, mean_q: 1.383573, mean_eps: 0.000000\n",
      " 1025/5000: episode: 146, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.162562, mean_squared_error: 0.752478, mean_q: 1.436391, mean_eps: 0.000000\n",
      " 1029/5000: episode: 147, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.145601, mean_squared_error: 0.808829, mean_q: 1.501994, mean_eps: 0.000000\n",
      " 1033/5000: episode: 148, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.122312, mean_squared_error: 0.821035, mean_q: 1.533472, mean_eps: 0.000000\n",
      " 1037/5000: episode: 149, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.115065, mean_squared_error: 0.849538, mean_q: 1.567453, mean_eps: 0.000000\n",
      " 1041/5000: episode: 150, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.116410, mean_squared_error: 0.833657, mean_q: 1.586532, mean_eps: 0.000000\n",
      " 1045/5000: episode: 151, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.094853, mean_squared_error: 0.877054, mean_q: 1.622676, mean_eps: 0.000000\n",
      " 1049/5000: episode: 152, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.097108, mean_squared_error: 0.832226, mean_q: 1.621642, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1053/5000: episode: 153, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.090670, mean_squared_error: 0.912915, mean_q: 1.635656, mean_eps: 0.000000\n",
      " 1063/5000: episode: 154, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 3.400, mean reward: 0.340 [-0.100, 1.000], mean action: 2.600 [1.000, 3.000], mean observation: 0.156 [0.000, 1.000], loss: 0.084477, mean_squared_error: 0.885038, mean_q: 1.666763, mean_eps: 0.000000\n",
      " 1067/5000: episode: 155, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.075051, mean_squared_error: 0.824632, mean_q: 1.655593, mean_eps: 0.000000\n",
      " 1071/5000: episode: 156, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.063105, mean_squared_error: 0.887102, mean_q: 1.673920, mean_eps: 0.000000\n",
      " 1075/5000: episode: 157, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.070615, mean_squared_error: 0.888959, mean_q: 1.678709, mean_eps: 0.000000\n",
      " 1079/5000: episode: 158, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.075857, mean_squared_error: 0.892214, mean_q: 1.691225, mean_eps: 0.000000\n",
      " 1083/5000: episode: 159, duration: 0.067s, episode steps: 4, steps per second: 60, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.055493, mean_squared_error: 0.916920, mean_q: 1.693010, mean_eps: 0.000000\n",
      " 1088/5000: episode: 160, duration: 0.069s, episode steps: 5, steps per second: 73, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 2.200 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.065960, mean_squared_error: 0.868354, mean_q: 1.656518, mean_eps: 0.000000\n",
      " 1099/5000: episode: 161, duration: 0.144s, episode steps: 11, steps per second: 76, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 2.818 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.067992, mean_squared_error: 0.861173, mean_q: 1.650145, mean_eps: 0.000000\n",
      " 1103/5000: episode: 162, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.081047, mean_squared_error: 0.890342, mean_q: 1.626808, mean_eps: 0.000000\n",
      " 1109/5000: episode: 163, duration: 0.080s, episode steps: 6, steps per second: 75, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.061652, mean_squared_error: 0.872700, mean_q: 1.629125, mean_eps: 0.000000\n",
      " 1113/5000: episode: 164, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.060838, mean_squared_error: 0.954793, mean_q: 1.674872, mean_eps: 0.000000\n",
      " 1118/5000: episode: 165, duration: 0.068s, episode steps: 5, steps per second: 74, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 2.200 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.049900, mean_squared_error: 0.834084, mean_q: 1.649007, mean_eps: 0.000000\n",
      " 1124/5000: episode: 166, duration: 0.080s, episode steps: 6, steps per second: 75, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.057808, mean_squared_error: 0.832938, mean_q: 1.651186, mean_eps: 0.000000\n",
      " 1135/5000: episode: 167, duration: 0.138s, episode steps: 11, steps per second: 80, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 2.818 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.055370, mean_squared_error: 0.824371, mean_q: 1.644376, mean_eps: 0.000000\n",
      " 1139/5000: episode: 168, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.048386, mean_squared_error: 0.821164, mean_q: 1.651501, mean_eps: 0.000000\n",
      " 1143/5000: episode: 169, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.048635, mean_squared_error: 0.853383, mean_q: 1.655387, mean_eps: 0.000000\n",
      " 1150/5000: episode: 170, duration: 0.094s, episode steps: 7, steps per second: 74, episode reward: 3.700, mean reward: 0.529 [-0.100, 1.000], mean action: 2.429 [1.000, 3.000], mean observation: 0.151 [0.000, 1.000], loss: 0.056755, mean_squared_error: 0.825549, mean_q: 1.666062, mean_eps: 0.000000\n",
      " 1161/5000: episode: 171, duration: 0.137s, episode steps: 11, steps per second: 80, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 2.818 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.056779, mean_squared_error: 0.816868, mean_q: 1.609187, mean_eps: 0.000000\n",
      " 1165/5000: episode: 172, duration: 0.066s, episode steps: 4, steps per second: 60, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.049338, mean_squared_error: 0.791533, mean_q: 1.613481, mean_eps: 0.000000\n",
      " 1176/5000: episode: 173, duration: 0.140s, episode steps: 11, steps per second: 78, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 2.818 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.049844, mean_squared_error: 0.809680, mean_q: 1.617950, mean_eps: 0.000000\n",
      " 1187/5000: episode: 174, duration: 0.141s, episode steps: 11, steps per second: 78, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 2.818 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.047192, mean_squared_error: 0.837807, mean_q: 1.640620, mean_eps: 0.000000\n",
      " 1193/5000: episode: 175, duration: 0.082s, episode steps: 6, steps per second: 73, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.050118, mean_squared_error: 0.781398, mean_q: 1.607918, mean_eps: 0.000000\n",
      " 1197/5000: episode: 176, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.060255, mean_squared_error: 0.818923, mean_q: 1.599987, mean_eps: 0.000000\n",
      " 1201/5000: episode: 177, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.043317, mean_squared_error: 0.762932, mean_q: 1.609714, mean_eps: 0.000000\n",
      " 1205/5000: episode: 178, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.051349, mean_squared_error: 0.834945, mean_q: 1.634571, mean_eps: 0.000000\n",
      " 1209/5000: episode: 179, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.036770, mean_squared_error: 0.744047, mean_q: 1.606082, mean_eps: 0.000000\n",
      " 1213/5000: episode: 180, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.043915, mean_squared_error: 0.817848, mean_q: 1.626096, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1217/5000: episode: 181, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.061532, mean_squared_error: 0.814422, mean_q: 1.602478, mean_eps: 0.000000\n",
      " 1221/5000: episode: 182, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.064564, mean_squared_error: 0.782434, mean_q: 1.604459, mean_eps: 0.000000\n",
      " 1225/5000: episode: 183, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.038220, mean_squared_error: 0.761748, mean_q: 1.595898, mean_eps: 0.000000\n",
      " 1229/5000: episode: 184, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.045783, mean_squared_error: 0.739686, mean_q: 1.576927, mean_eps: 0.000000\n",
      " 1233/5000: episode: 185, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.045854, mean_squared_error: 0.778838, mean_q: 1.599402, mean_eps: 0.000000\n",
      " 1237/5000: episode: 186, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.036121, mean_squared_error: 0.802288, mean_q: 1.632191, mean_eps: 0.000000\n",
      " 1241/5000: episode: 187, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.043469, mean_squared_error: 0.810597, mean_q: 1.616175, mean_eps: 0.000000\n",
      " 1246/5000: episode: 188, duration: 0.066s, episode steps: 5, steps per second: 76, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 2.200 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.035631, mean_squared_error: 0.757490, mean_q: 1.622251, mean_eps: 0.000000\n",
      " 1250/5000: episode: 189, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.037672, mean_squared_error: 0.815873, mean_q: 1.626816, mean_eps: 0.000000\n",
      " 1254/5000: episode: 190, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.034590, mean_squared_error: 0.815662, mean_q: 1.662781, mean_eps: 0.000000\n",
      " 1258/5000: episode: 191, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.037443, mean_squared_error: 0.784361, mean_q: 1.644292, mean_eps: 0.000000\n",
      " 1262/5000: episode: 192, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.024574, mean_squared_error: 0.757452, mean_q: 1.620917, mean_eps: 0.000000\n",
      " 1266/5000: episode: 193, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.024762, mean_squared_error: 0.774779, mean_q: 1.631766, mean_eps: 0.000000\n",
      " 1270/5000: episode: 194, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.023842, mean_squared_error: 0.768140, mean_q: 1.668584, mean_eps: 0.000000\n",
      " 1274/5000: episode: 195, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.029097, mean_squared_error: 0.698603, mean_q: 1.594610, mean_eps: 0.000000\n",
      " 1278/5000: episode: 196, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.027763, mean_squared_error: 0.756128, mean_q: 1.621607, mean_eps: 0.000000\n",
      " 1282/5000: episode: 197, duration: 0.066s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.033708, mean_squared_error: 0.734240, mean_q: 1.611553, mean_eps: 0.000000\n",
      " 1286/5000: episode: 198, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.029663, mean_squared_error: 0.657649, mean_q: 1.538541, mean_eps: 0.000000\n",
      " 1290/5000: episode: 199, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.024604, mean_squared_error: 0.689255, mean_q: 1.610171, mean_eps: 0.000000\n",
      " 1294/5000: episode: 200, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.031764, mean_squared_error: 0.688546, mean_q: 1.591497, mean_eps: 0.000000\n",
      " 1298/5000: episode: 201, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.031056, mean_squared_error: 0.726185, mean_q: 1.662109, mean_eps: 0.000000\n",
      " 1302/5000: episode: 202, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.020127, mean_squared_error: 0.694921, mean_q: 1.631998, mean_eps: 0.000000\n",
      " 1306/5000: episode: 203, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.031479, mean_squared_error: 0.748896, mean_q: 1.598444, mean_eps: 0.000000\n",
      " 1310/5000: episode: 204, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.032168, mean_squared_error: 0.736527, mean_q: 1.602842, mean_eps: 0.000000\n",
      " 1314/5000: episode: 205, duration: 0.066s, episode steps: 4, steps per second: 60, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.021196, mean_squared_error: 0.722892, mean_q: 1.616143, mean_eps: 0.000000\n",
      " 1318/5000: episode: 206, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.017900, mean_squared_error: 0.717780, mean_q: 1.625364, mean_eps: 0.000000\n",
      " 1322/5000: episode: 207, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.024797, mean_squared_error: 0.746848, mean_q: 1.641017, mean_eps: 0.000000\n",
      " 1326/5000: episode: 208, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.029725, mean_squared_error: 0.720443, mean_q: 1.621163, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1330/5000: episode: 209, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.017294, mean_squared_error: 0.695171, mean_q: 1.613278, mean_eps: 0.000000\n",
      " 1334/5000: episode: 210, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.026526, mean_squared_error: 0.769100, mean_q: 1.664147, mean_eps: 0.000000\n",
      " 1338/5000: episode: 211, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.014616, mean_squared_error: 0.757253, mean_q: 1.640741, mean_eps: 0.000000\n",
      " 1342/5000: episode: 212, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.020585, mean_squared_error: 0.717252, mean_q: 1.662748, mean_eps: 0.000000\n",
      " 1346/5000: episode: 213, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.013305, mean_squared_error: 0.685159, mean_q: 1.645814, mean_eps: 0.000000\n",
      " 1350/5000: episode: 214, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.022536, mean_squared_error: 0.717496, mean_q: 1.630084, mean_eps: 0.000000\n",
      " 1354/5000: episode: 215, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.024341, mean_squared_error: 0.669283, mean_q: 1.586338, mean_eps: 0.000000\n",
      " 1358/5000: episode: 216, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.023585, mean_squared_error: 0.668334, mean_q: 1.573606, mean_eps: 0.000000\n",
      " 1362/5000: episode: 217, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.012500, mean_squared_error: 0.725657, mean_q: 1.631036, mean_eps: 0.000000\n",
      " 1366/5000: episode: 218, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.016433, mean_squared_error: 0.677544, mean_q: 1.607288, mean_eps: 0.000000\n",
      " 1370/5000: episode: 219, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010115, mean_squared_error: 0.629617, mean_q: 1.558792, mean_eps: 0.000000\n",
      " 1374/5000: episode: 220, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.022727, mean_squared_error: 0.710215, mean_q: 1.674540, mean_eps: 0.000000\n",
      " 1378/5000: episode: 221, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.013324, mean_squared_error: 0.722054, mean_q: 1.613198, mean_eps: 0.000000\n",
      " 1382/5000: episode: 222, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.020430, mean_squared_error: 0.651404, mean_q: 1.606832, mean_eps: 0.000000\n",
      " 1386/5000: episode: 223, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.019885, mean_squared_error: 0.756978, mean_q: 1.679636, mean_eps: 0.000000\n",
      " 1390/5000: episode: 224, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.018867, mean_squared_error: 0.690423, mean_q: 1.655747, mean_eps: 0.000000\n",
      " 1394/5000: episode: 225, duration: 0.054s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006059, mean_squared_error: 0.686240, mean_q: 1.648465, mean_eps: 0.000000\n",
      " 1398/5000: episode: 226, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.016024, mean_squared_error: 0.747056, mean_q: 1.694648, mean_eps: 0.000000\n",
      " 1402/5000: episode: 227, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.013607, mean_squared_error: 0.704819, mean_q: 1.647223, mean_eps: 0.000000\n",
      " 1406/5000: episode: 228, duration: 0.065s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.013480, mean_squared_error: 0.688315, mean_q: 1.673754, mean_eps: 0.000000\n",
      " 1410/5000: episode: 229, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.031458, mean_squared_error: 0.701178, mean_q: 1.651092, mean_eps: 0.000000\n",
      " 1414/5000: episode: 230, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010935, mean_squared_error: 0.701298, mean_q: 1.650748, mean_eps: 0.000000\n",
      " 1418/5000: episode: 231, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.012667, mean_squared_error: 0.656935, mean_q: 1.566374, mean_eps: 0.000000\n",
      " 1422/5000: episode: 232, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.013022, mean_squared_error: 0.691316, mean_q: 1.628737, mean_eps: 0.000000\n",
      " 1426/5000: episode: 233, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.015850, mean_squared_error: 0.693030, mean_q: 1.627402, mean_eps: 0.000000\n",
      " 1430/5000: episode: 234, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.012744, mean_squared_error: 0.682405, mean_q: 1.648447, mean_eps: 0.000000\n",
      " 1434/5000: episode: 235, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.013146, mean_squared_error: 0.681479, mean_q: 1.612633, mean_eps: 0.000000\n",
      " 1438/5000: episode: 236, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.009577, mean_squared_error: 0.684407, mean_q: 1.638723, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1442/5000: episode: 237, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007412, mean_squared_error: 0.604350, mean_q: 1.547955, mean_eps: 0.000000\n",
      " 1446/5000: episode: 238, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010773, mean_squared_error: 0.625428, mean_q: 1.593914, mean_eps: 0.000000\n",
      " 1450/5000: episode: 239, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.011025, mean_squared_error: 0.676281, mean_q: 1.624455, mean_eps: 0.000000\n",
      " 1454/5000: episode: 240, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.017835, mean_squared_error: 0.652634, mean_q: 1.598846, mean_eps: 0.000000\n",
      " 1458/5000: episode: 241, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006790, mean_squared_error: 0.729271, mean_q: 1.659112, mean_eps: 0.000000\n",
      " 1462/5000: episode: 242, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.016783, mean_squared_error: 0.695868, mean_q: 1.632552, mean_eps: 0.000000\n",
      " 1466/5000: episode: 243, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.017178, mean_squared_error: 0.661453, mean_q: 1.578988, mean_eps: 0.000000\n",
      " 1470/5000: episode: 244, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010195, mean_squared_error: 0.641270, mean_q: 1.551861, mean_eps: 0.000000\n",
      " 1474/5000: episode: 245, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.009780, mean_squared_error: 0.674666, mean_q: 1.625506, mean_eps: 0.000000\n",
      " 1478/5000: episode: 246, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.009170, mean_squared_error: 0.689111, mean_q: 1.683674, mean_eps: 0.000000\n",
      " 1482/5000: episode: 247, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004815, mean_squared_error: 0.684972, mean_q: 1.643523, mean_eps: 0.000000\n",
      " 1486/5000: episode: 248, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007210, mean_squared_error: 0.668859, mean_q: 1.596031, mean_eps: 0.000000\n",
      " 1490/5000: episode: 249, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.016752, mean_squared_error: 0.690144, mean_q: 1.642618, mean_eps: 0.000000\n",
      " 1494/5000: episode: 250, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010550, mean_squared_error: 0.731081, mean_q: 1.686345, mean_eps: 0.000000\n",
      " 1498/5000: episode: 251, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007845, mean_squared_error: 0.684378, mean_q: 1.653628, mean_eps: 0.000000\n",
      " 1502/5000: episode: 252, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003596, mean_squared_error: 0.667247, mean_q: 1.631337, mean_eps: 0.000000\n",
      " 1506/5000: episode: 253, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002580, mean_squared_error: 0.689293, mean_q: 1.673793, mean_eps: 0.000000\n",
      " 1510/5000: episode: 254, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.009319, mean_squared_error: 0.681347, mean_q: 1.655147, mean_eps: 0.000000\n",
      " 1514/5000: episode: 255, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003333, mean_squared_error: 0.742942, mean_q: 1.690321, mean_eps: 0.000000\n",
      " 1518/5000: episode: 256, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002786, mean_squared_error: 0.672891, mean_q: 1.618964, mean_eps: 0.000000\n",
      " 1522/5000: episode: 257, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007140, mean_squared_error: 0.657534, mean_q: 1.610375, mean_eps: 0.000000\n",
      " 1526/5000: episode: 258, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.014523, mean_squared_error: 0.696971, mean_q: 1.643336, mean_eps: 0.000000\n",
      " 1530/5000: episode: 259, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.008654, mean_squared_error: 0.656889, mean_q: 1.597188, mean_eps: 0.000000\n",
      " 1534/5000: episode: 260, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002598, mean_squared_error: 0.695124, mean_q: 1.652720, mean_eps: 0.000000\n",
      " 1538/5000: episode: 261, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003357, mean_squared_error: 0.667816, mean_q: 1.639296, mean_eps: 0.000000\n",
      " 1542/5000: episode: 262, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000789, mean_squared_error: 0.647561, mean_q: 1.608794, mean_eps: 0.000000\n",
      " 1546/5000: episode: 263, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.008782, mean_squared_error: 0.661470, mean_q: 1.629315, mean_eps: 0.000000\n",
      " 1550/5000: episode: 264, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001399, mean_squared_error: 0.673287, mean_q: 1.657591, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1554/5000: episode: 265, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001289, mean_squared_error: 0.660241, mean_q: 1.661619, mean_eps: 0.000000\n",
      " 1558/5000: episode: 266, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003759, mean_squared_error: 0.634628, mean_q: 1.586773, mean_eps: 0.000000\n",
      " 1562/5000: episode: 267, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.011439, mean_squared_error: 0.697556, mean_q: 1.687298, mean_eps: 0.000000\n",
      " 1566/5000: episode: 268, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001004, mean_squared_error: 0.653890, mean_q: 1.641516, mean_eps: 0.000000\n",
      " 1570/5000: episode: 269, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001350, mean_squared_error: 0.690809, mean_q: 1.685449, mean_eps: 0.000000\n",
      " 1574/5000: episode: 270, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001157, mean_squared_error: 0.666410, mean_q: 1.647288, mean_eps: 0.000000\n",
      " 1578/5000: episode: 271, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001605, mean_squared_error: 0.613977, mean_q: 1.596601, mean_eps: 0.000000\n",
      " 1582/5000: episode: 272, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002213, mean_squared_error: 0.741330, mean_q: 1.749754, mean_eps: 0.000000\n",
      " 1586/5000: episode: 273, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004938, mean_squared_error: 0.661097, mean_q: 1.636764, mean_eps: 0.000000\n",
      " 1590/5000: episode: 274, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001548, mean_squared_error: 0.645116, mean_q: 1.614572, mean_eps: 0.000000\n",
      " 1594/5000: episode: 275, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004589, mean_squared_error: 0.687261, mean_q: 1.653584, mean_eps: 0.000000\n",
      " 1598/5000: episode: 276, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004345, mean_squared_error: 0.659078, mean_q: 1.650134, mean_eps: 0.000000\n",
      " 1602/5000: episode: 277, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002267, mean_squared_error: 0.720972, mean_q: 1.693216, mean_eps: 0.000000\n",
      " 1606/5000: episode: 278, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004302, mean_squared_error: 0.646703, mean_q: 1.628976, mean_eps: 0.000000\n",
      " 1610/5000: episode: 279, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004564, mean_squared_error: 0.663368, mean_q: 1.650647, mean_eps: 0.000000\n",
      " 1614/5000: episode: 280, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001923, mean_squared_error: 0.666270, mean_q: 1.617867, mean_eps: 0.000000\n",
      " 1618/5000: episode: 281, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004882, mean_squared_error: 0.658282, mean_q: 1.648193, mean_eps: 0.000000\n",
      " 1622/5000: episode: 282, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001621, mean_squared_error: 0.679946, mean_q: 1.672106, mean_eps: 0.000000\n",
      " 1626/5000: episode: 283, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002157, mean_squared_error: 0.708011, mean_q: 1.676456, mean_eps: 0.000000\n",
      " 1630/5000: episode: 284, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000857, mean_squared_error: 0.617752, mean_q: 1.593025, mean_eps: 0.000000\n",
      " 1634/5000: episode: 285, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004600, mean_squared_error: 0.702360, mean_q: 1.668831, mean_eps: 0.000000\n",
      " 1638/5000: episode: 286, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.009913, mean_squared_error: 0.616930, mean_q: 1.574013, mean_eps: 0.000000\n",
      " 1642/5000: episode: 287, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001280, mean_squared_error: 0.706486, mean_q: 1.669576, mean_eps: 0.000000\n",
      " 1646/5000: episode: 288, duration: 0.054s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003865, mean_squared_error: 0.737058, mean_q: 1.723987, mean_eps: 0.000000\n",
      " 1650/5000: episode: 289, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001779, mean_squared_error: 0.680994, mean_q: 1.677763, mean_eps: 0.000000\n",
      " 1654/5000: episode: 290, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004042, mean_squared_error: 0.668415, mean_q: 1.629632, mean_eps: 0.000000\n",
      " 1658/5000: episode: 291, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005818, mean_squared_error: 0.701303, mean_q: 1.670915, mean_eps: 0.000000\n",
      " 1662/5000: episode: 292, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005230, mean_squared_error: 0.747415, mean_q: 1.727318, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1666/5000: episode: 293, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000407, mean_squared_error: 0.690741, mean_q: 1.662091, mean_eps: 0.000000\n",
      " 1670/5000: episode: 294, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000166, mean_squared_error: 0.717354, mean_q: 1.711493, mean_eps: 0.000000\n",
      " 1674/5000: episode: 295, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.009406, mean_squared_error: 0.634470, mean_q: 1.590350, mean_eps: 0.000000\n",
      " 1678/5000: episode: 296, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004491, mean_squared_error: 0.725204, mean_q: 1.709332, mean_eps: 0.000000\n",
      " 1682/5000: episode: 297, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003477, mean_squared_error: 0.699992, mean_q: 1.686854, mean_eps: 0.000000\n",
      " 1686/5000: episode: 298, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003616, mean_squared_error: 0.721237, mean_q: 1.693083, mean_eps: 0.000000\n",
      " 1690/5000: episode: 299, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006603, mean_squared_error: 0.722483, mean_q: 1.699666, mean_eps: 0.000000\n",
      " 1694/5000: episode: 300, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003171, mean_squared_error: 0.640467, mean_q: 1.626374, mean_eps: 0.000000\n",
      " 1698/5000: episode: 301, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005923, mean_squared_error: 0.645653, mean_q: 1.628950, mean_eps: 0.000000\n",
      " 1702/5000: episode: 302, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003099, mean_squared_error: 0.682982, mean_q: 1.649773, mean_eps: 0.000000\n",
      " 1706/5000: episode: 303, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000261, mean_squared_error: 0.649806, mean_q: 1.628837, mean_eps: 0.000000\n",
      " 1710/5000: episode: 304, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006203, mean_squared_error: 0.652273, mean_q: 1.618415, mean_eps: 0.000000\n",
      " 1714/5000: episode: 305, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003359, mean_squared_error: 0.696522, mean_q: 1.668970, mean_eps: 0.000000\n",
      " 1718/5000: episode: 306, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000702, mean_squared_error: 0.699999, mean_q: 1.695346, mean_eps: 0.000000\n",
      " 1722/5000: episode: 307, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005870, mean_squared_error: 0.658780, mean_q: 1.613404, mean_eps: 0.000000\n",
      " 1726/5000: episode: 308, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006114, mean_squared_error: 0.687788, mean_q: 1.660830, mean_eps: 0.000000\n",
      " 1730/5000: episode: 309, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003613, mean_squared_error: 0.668689, mean_q: 1.634437, mean_eps: 0.000000\n",
      " 1734/5000: episode: 310, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000538, mean_squared_error: 0.654232, mean_q: 1.625755, mean_eps: 0.000000\n",
      " 1738/5000: episode: 311, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000353, mean_squared_error: 0.717973, mean_q: 1.712400, mean_eps: 0.000000\n",
      " 1742/5000: episode: 312, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000500, mean_squared_error: 0.663149, mean_q: 1.640653, mean_eps: 0.000000\n",
      " 1746/5000: episode: 313, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003370, mean_squared_error: 0.697279, mean_q: 1.688314, mean_eps: 0.000000\n",
      " 1750/5000: episode: 314, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000221, mean_squared_error: 0.708535, mean_q: 1.712926, mean_eps: 0.000000\n",
      " 1754/5000: episode: 315, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002890, mean_squared_error: 0.672085, mean_q: 1.649684, mean_eps: 0.000000\n",
      " 1758/5000: episode: 316, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005686, mean_squared_error: 0.649211, mean_q: 1.602675, mean_eps: 0.000000\n",
      " 1762/5000: episode: 317, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000626, mean_squared_error: 0.662412, mean_q: 1.655180, mean_eps: 0.000000\n",
      " 1766/5000: episode: 318, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000304, mean_squared_error: 0.726768, mean_q: 1.716332, mean_eps: 0.000000\n",
      " 1770/5000: episode: 319, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000220, mean_squared_error: 0.649742, mean_q: 1.604140, mean_eps: 0.000000\n",
      " 1774/5000: episode: 320, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.012167, mean_squared_error: 0.676206, mean_q: 1.652703, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1778/5000: episode: 321, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001173, mean_squared_error: 0.693242, mean_q: 1.663597, mean_eps: 0.000000\n",
      " 1782/5000: episode: 322, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002918, mean_squared_error: 0.686519, mean_q: 1.651180, mean_eps: 0.000000\n",
      " 1786/5000: episode: 323, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000422, mean_squared_error: 0.713193, mean_q: 1.714087, mean_eps: 0.000000\n",
      " 1790/5000: episode: 324, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000278, mean_squared_error: 0.711126, mean_q: 1.699929, mean_eps: 0.000000\n",
      " 1794/5000: episode: 325, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003323, mean_squared_error: 0.708083, mean_q: 1.692682, mean_eps: 0.000000\n",
      " 1798/5000: episode: 326, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003132, mean_squared_error: 0.629751, mean_q: 1.582546, mean_eps: 0.000000\n",
      " 1802/5000: episode: 327, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000549, mean_squared_error: 0.690874, mean_q: 1.671996, mean_eps: 0.000000\n",
      " 1806/5000: episode: 328, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003109, mean_squared_error: 0.663883, mean_q: 1.644803, mean_eps: 0.000000\n",
      " 1810/5000: episode: 329, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006167, mean_squared_error: 0.654629, mean_q: 1.658502, mean_eps: 0.000000\n",
      " 1814/5000: episode: 330, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000519, mean_squared_error: 0.679345, mean_q: 1.664912, mean_eps: 0.000000\n",
      " 1818/5000: episode: 331, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002899, mean_squared_error: 0.629102, mean_q: 1.595096, mean_eps: 0.000000\n",
      " 1822/5000: episode: 332, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005964, mean_squared_error: 0.704583, mean_q: 1.692178, mean_eps: 0.000000\n",
      " 1826/5000: episode: 333, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000323, mean_squared_error: 0.697347, mean_q: 1.687678, mean_eps: 0.000000\n",
      " 1830/5000: episode: 334, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000419, mean_squared_error: 0.719427, mean_q: 1.720928, mean_eps: 0.000000\n",
      " 1834/5000: episode: 335, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005872, mean_squared_error: 0.654096, mean_q: 1.622855, mean_eps: 0.000000\n",
      " 1838/5000: episode: 336, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005624, mean_squared_error: 0.656629, mean_q: 1.631907, mean_eps: 0.000000\n",
      " 1842/5000: episode: 337, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000238, mean_squared_error: 0.700743, mean_q: 1.650800, mean_eps: 0.000000\n",
      " 1846/5000: episode: 338, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.008433, mean_squared_error: 0.647006, mean_q: 1.618139, mean_eps: 0.000000\n",
      " 1850/5000: episode: 339, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002886, mean_squared_error: 0.671748, mean_q: 1.642893, mean_eps: 0.000000\n",
      " 1854/5000: episode: 340, duration: 0.052s, episode steps: 4, steps per second: 76, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003046, mean_squared_error: 0.686360, mean_q: 1.663849, mean_eps: 0.000000\n",
      " 1858/5000: episode: 341, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007958, mean_squared_error: 0.656015, mean_q: 1.625705, mean_eps: 0.000000\n",
      " 1862/5000: episode: 342, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000465, mean_squared_error: 0.718177, mean_q: 1.701763, mean_eps: 0.000000\n",
      " 1866/5000: episode: 343, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000627, mean_squared_error: 0.635012, mean_q: 1.608391, mean_eps: 0.000000\n",
      " 1870/5000: episode: 344, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000129, mean_squared_error: 0.713566, mean_q: 1.727551, mean_eps: 0.000000\n",
      " 1874/5000: episode: 345, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000105, mean_squared_error: 0.744712, mean_q: 1.744370, mean_eps: 0.000000\n",
      " 1878/5000: episode: 346, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000144, mean_squared_error: 0.708258, mean_q: 1.701360, mean_eps: 0.000000\n",
      " 1882/5000: episode: 347, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000123, mean_squared_error: 0.724370, mean_q: 1.710899, mean_eps: 0.000000\n",
      " 1886/5000: episode: 348, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000131, mean_squared_error: 0.689653, mean_q: 1.667653, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1890/5000: episode: 349, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000067, mean_squared_error: 0.656706, mean_q: 1.651193, mean_eps: 0.000000\n",
      " 1894/5000: episode: 350, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000163, mean_squared_error: 0.673662, mean_q: 1.633053, mean_eps: 0.000000\n",
      " 1898/5000: episode: 351, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002815, mean_squared_error: 0.711765, mean_q: 1.689369, mean_eps: 0.000000\n",
      " 1902/5000: episode: 352, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000110, mean_squared_error: 0.679106, mean_q: 1.667926, mean_eps: 0.000000\n",
      " 1906/5000: episode: 353, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002820, mean_squared_error: 0.677378, mean_q: 1.661164, mean_eps: 0.000000\n",
      " 1910/5000: episode: 354, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000080, mean_squared_error: 0.644904, mean_q: 1.617189, mean_eps: 0.000000\n",
      " 1914/5000: episode: 355, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000076, mean_squared_error: 0.666625, mean_q: 1.635698, mean_eps: 0.000000\n",
      " 1918/5000: episode: 356, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000046, mean_squared_error: 0.655629, mean_q: 1.641703, mean_eps: 0.000000\n",
      " 1922/5000: episode: 357, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000099, mean_squared_error: 0.707898, mean_q: 1.689322, mean_eps: 0.000000\n",
      " 1926/5000: episode: 358, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002868, mean_squared_error: 0.671511, mean_q: 1.635496, mean_eps: 0.000000\n",
      " 1930/5000: episode: 359, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000124, mean_squared_error: 0.676916, mean_q: 1.662779, mean_eps: 0.000000\n",
      " 1934/5000: episode: 360, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000116, mean_squared_error: 0.672236, mean_q: 1.646854, mean_eps: 0.000000\n",
      " 1938/5000: episode: 361, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000061, mean_squared_error: 0.688732, mean_q: 1.681690, mean_eps: 0.000000\n",
      " 1942/5000: episode: 362, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002708, mean_squared_error: 0.677112, mean_q: 1.682684, mean_eps: 0.000000\n",
      " 1946/5000: episode: 363, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002757, mean_squared_error: 0.690921, mean_q: 1.673503, mean_eps: 0.000000\n",
      " 1950/5000: episode: 364, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005395, mean_squared_error: 0.629657, mean_q: 1.591448, mean_eps: 0.000000\n",
      " 1954/5000: episode: 365, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002689, mean_squared_error: 0.735346, mean_q: 1.736027, mean_eps: 0.000000\n",
      " 1958/5000: episode: 366, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002722, mean_squared_error: 0.702410, mean_q: 1.687663, mean_eps: 0.000000\n",
      " 1962/5000: episode: 367, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000117, mean_squared_error: 0.733249, mean_q: 1.722846, mean_eps: 0.000000\n",
      " 1966/5000: episode: 368, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000054, mean_squared_error: 0.701441, mean_q: 1.693243, mean_eps: 0.000000\n",
      " 1970/5000: episode: 369, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000043, mean_squared_error: 0.676316, mean_q: 1.679559, mean_eps: 0.000000\n",
      " 1974/5000: episode: 370, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000042, mean_squared_error: 0.722270, mean_q: 1.724289, mean_eps: 0.000000\n",
      " 1978/5000: episode: 371, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005213, mean_squared_error: 0.740923, mean_q: 1.759530, mean_eps: 0.000000\n",
      " 1982/5000: episode: 372, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000020, mean_squared_error: 0.646874, mean_q: 1.628268, mean_eps: 0.000000\n",
      " 1986/5000: episode: 373, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000055, mean_squared_error: 0.673824, mean_q: 1.661208, mean_eps: 0.000000\n",
      " 1990/5000: episode: 374, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 0.729727, mean_q: 1.741801, mean_eps: 0.000000\n",
      " 1994/5000: episode: 375, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 0.713539, mean_q: 1.720331, mean_eps: 0.000000\n",
      " 1998/5000: episode: 376, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 0.704938, mean_q: 1.674501, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2002/5000: episode: 377, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.061249, mean_squared_error: 0.755720, mean_q: 1.740407, mean_eps: 0.000000\n",
      " 2006/5000: episode: 378, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.136942, mean_squared_error: 0.912500, mean_q: 1.893277, mean_eps: 0.000000\n",
      " 2010/5000: episode: 379, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.086476, mean_squared_error: 0.955326, mean_q: 1.957108, mean_eps: 0.000000\n",
      " 2014/5000: episode: 380, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.077451, mean_squared_error: 1.081368, mean_q: 2.074009, mean_eps: 0.000000\n",
      " 2018/5000: episode: 381, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.064976, mean_squared_error: 1.102316, mean_q: 2.125448, mean_eps: 0.000000\n",
      " 2022/5000: episode: 382, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.057182, mean_squared_error: 1.162023, mean_q: 2.157497, mean_eps: 0.000000\n",
      " 2026/5000: episode: 383, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.051953, mean_squared_error: 1.142949, mean_q: 2.155301, mean_eps: 0.000000\n",
      " 2030/5000: episode: 384, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.046628, mean_squared_error: 1.097958, mean_q: 2.098487, mean_eps: 0.000000\n",
      " 2034/5000: episode: 385, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.042025, mean_squared_error: 1.251344, mean_q: 2.252645, mean_eps: 0.000000\n",
      " 2038/5000: episode: 386, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.033343, mean_squared_error: 1.199052, mean_q: 2.209221, mean_eps: 0.000000\n",
      " 2042/5000: episode: 387, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.028829, mean_squared_error: 1.160648, mean_q: 2.192415, mean_eps: 0.000000\n",
      " 2046/5000: episode: 388, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.027529, mean_squared_error: 1.074695, mean_q: 2.116734, mean_eps: 0.000000\n",
      " 2050/5000: episode: 389, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.025535, mean_squared_error: 1.109510, mean_q: 2.121919, mean_eps: 0.000000\n",
      " 2054/5000: episode: 390, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.020900, mean_squared_error: 1.225761, mean_q: 2.252677, mean_eps: 0.000000\n",
      " 2058/5000: episode: 391, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.018606, mean_squared_error: 1.098808, mean_q: 2.139528, mean_eps: 0.000000\n",
      " 2062/5000: episode: 392, duration: 0.074s, episode steps: 4, steps per second: 54, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.018748, mean_squared_error: 1.159770, mean_q: 2.191668, mean_eps: 0.000000\n",
      " 2066/5000: episode: 393, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.016685, mean_squared_error: 1.188395, mean_q: 2.209574, mean_eps: 0.000000\n",
      " 2070/5000: episode: 394, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.014401, mean_squared_error: 1.148629, mean_q: 2.188521, mean_eps: 0.000000\n",
      " 2074/5000: episode: 395, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.012879, mean_squared_error: 1.190570, mean_q: 2.240571, mean_eps: 0.000000\n",
      " 2078/5000: episode: 396, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.012482, mean_squared_error: 1.127088, mean_q: 2.171320, mean_eps: 0.000000\n",
      " 2082/5000: episode: 397, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.012099, mean_squared_error: 1.191954, mean_q: 2.227900, mean_eps: 0.000000\n",
      " 2086/5000: episode: 398, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.009336, mean_squared_error: 1.167381, mean_q: 2.191443, mean_eps: 0.000000\n",
      " 2090/5000: episode: 399, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010034, mean_squared_error: 1.187769, mean_q: 2.225561, mean_eps: 0.000000\n",
      " 2094/5000: episode: 400, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010943, mean_squared_error: 1.099625, mean_q: 2.125185, mean_eps: 0.000000\n",
      " 2098/5000: episode: 401, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006542, mean_squared_error: 1.112810, mean_q: 2.130081, mean_eps: 0.000000\n",
      " 2102/5000: episode: 402, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.008682, mean_squared_error: 1.191918, mean_q: 2.251176, mean_eps: 0.000000\n",
      " 2106/5000: episode: 403, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006282, mean_squared_error: 1.118482, mean_q: 2.137774, mean_eps: 0.000000\n",
      " 2110/5000: episode: 404, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.008137, mean_squared_error: 1.081628, mean_q: 2.109880, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2114/5000: episode: 405, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004572, mean_squared_error: 1.262326, mean_q: 2.332437, mean_eps: 0.000000\n",
      " 2118/5000: episode: 406, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006459, mean_squared_error: 1.167988, mean_q: 2.204559, mean_eps: 0.000000\n",
      " 2122/5000: episode: 407, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003482, mean_squared_error: 1.105707, mean_q: 2.125342, mean_eps: 0.000000\n",
      " 2126/5000: episode: 408, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.008619, mean_squared_error: 1.236577, mean_q: 2.289737, mean_eps: 0.000000\n",
      " 2130/5000: episode: 409, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005471, mean_squared_error: 1.161163, mean_q: 2.213862, mean_eps: 0.000000\n",
      " 2134/5000: episode: 410, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004115, mean_squared_error: 1.144889, mean_q: 2.182118, mean_eps: 0.000000\n",
      " 2138/5000: episode: 411, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004681, mean_squared_error: 1.190186, mean_q: 2.243839, mean_eps: 0.000000\n",
      " 2142/5000: episode: 412, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006529, mean_squared_error: 1.156173, mean_q: 2.215957, mean_eps: 0.000000\n",
      " 2146/5000: episode: 413, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006441, mean_squared_error: 1.210238, mean_q: 2.270335, mean_eps: 0.000000\n",
      " 2150/5000: episode: 414, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003349, mean_squared_error: 1.102640, mean_q: 2.141065, mean_eps: 0.000000\n",
      " 2154/5000: episode: 415, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.005369, mean_squared_error: 1.138056, mean_q: 2.181622, mean_eps: 0.000000\n",
      " 2158/5000: episode: 416, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006355, mean_squared_error: 1.082028, mean_q: 2.117743, mean_eps: 0.000000\n",
      " 2162/5000: episode: 417, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003894, mean_squared_error: 1.186898, mean_q: 2.256586, mean_eps: 0.000000\n",
      " 2166/5000: episode: 418, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004480, mean_squared_error: 1.059415, mean_q: 2.080450, mean_eps: 0.000000\n",
      " 2170/5000: episode: 419, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001946, mean_squared_error: 1.172893, mean_q: 2.241738, mean_eps: 0.000000\n",
      " 2174/5000: episode: 420, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004055, mean_squared_error: 1.160793, mean_q: 2.229862, mean_eps: 0.000000\n",
      " 2178/5000: episode: 421, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003720, mean_squared_error: 1.216268, mean_q: 2.297403, mean_eps: 0.000000\n",
      " 2182/5000: episode: 422, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003465, mean_squared_error: 1.146878, mean_q: 2.206269, mean_eps: 0.000000\n",
      " 2186/5000: episode: 423, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002452, mean_squared_error: 1.259819, mean_q: 2.351085, mean_eps: 0.000000\n",
      " 2190/5000: episode: 424, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003784, mean_squared_error: 1.202979, mean_q: 2.299353, mean_eps: 0.000000\n",
      " 2194/5000: episode: 425, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003958, mean_squared_error: 1.201474, mean_q: 2.293354, mean_eps: 0.000000\n",
      " 2198/5000: episode: 426, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002209, mean_squared_error: 1.159714, mean_q: 2.240119, mean_eps: 0.000000\n",
      " 2202/5000: episode: 427, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003639, mean_squared_error: 1.132353, mean_q: 2.212513, mean_eps: 0.000000\n",
      " 2206/5000: episode: 428, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002688, mean_squared_error: 1.210909, mean_q: 2.295113, mean_eps: 0.000000\n",
      " 2210/5000: episode: 429, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003969, mean_squared_error: 1.158041, mean_q: 2.232230, mean_eps: 0.000000\n",
      " 2214/5000: episode: 430, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001420, mean_squared_error: 1.132986, mean_q: 2.191462, mean_eps: 0.000000\n",
      " 2218/5000: episode: 431, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001349, mean_squared_error: 1.095715, mean_q: 2.153019, mean_eps: 0.000000\n",
      " 2222/5000: episode: 432, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002474, mean_squared_error: 1.105997, mean_q: 2.154986, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2226/5000: episode: 433, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002161, mean_squared_error: 1.129991, mean_q: 2.203868, mean_eps: 0.000000\n",
      " 2230/5000: episode: 434, duration: 0.067s, episode steps: 4, steps per second: 59, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002601, mean_squared_error: 1.145786, mean_q: 2.212767, mean_eps: 0.000000\n",
      " 2234/5000: episode: 435, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003429, mean_squared_error: 1.131673, mean_q: 2.195632, mean_eps: 0.000000\n",
      " 2238/5000: episode: 436, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003576, mean_squared_error: 1.164796, mean_q: 2.243277, mean_eps: 0.000000\n",
      " 2242/5000: episode: 437, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002128, mean_squared_error: 1.104383, mean_q: 2.163080, mean_eps: 0.000000\n",
      " 2246/5000: episode: 438, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002032, mean_squared_error: 1.203786, mean_q: 2.290303, mean_eps: 0.000000\n",
      " 2250/5000: episode: 439, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002267, mean_squared_error: 1.099331, mean_q: 2.174315, mean_eps: 0.000000\n",
      " 2254/5000: episode: 440, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001809, mean_squared_error: 1.121764, mean_q: 2.196302, mean_eps: 0.000000\n",
      " 2258/5000: episode: 441, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001465, mean_squared_error: 1.118831, mean_q: 2.183955, mean_eps: 0.000000\n",
      " 2262/5000: episode: 442, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001978, mean_squared_error: 1.169694, mean_q: 2.249284, mean_eps: 0.000000\n",
      " 2266/5000: episode: 443, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001348, mean_squared_error: 1.190614, mean_q: 2.276076, mean_eps: 0.000000\n",
      " 2270/5000: episode: 444, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001237, mean_squared_error: 1.132483, mean_q: 2.194114, mean_eps: 0.000000\n",
      " 2274/5000: episode: 445, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001900, mean_squared_error: 1.078239, mean_q: 2.135468, mean_eps: 0.000000\n",
      " 2278/5000: episode: 446, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001568, mean_squared_error: 1.140725, mean_q: 2.227616, mean_eps: 0.000000\n",
      " 2282/5000: episode: 447, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001508, mean_squared_error: 1.076491, mean_q: 2.128160, mean_eps: 0.000000\n",
      " 2286/5000: episode: 448, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000816, mean_squared_error: 1.168157, mean_q: 2.259858, mean_eps: 0.000000\n",
      " 2290/5000: episode: 449, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000934, mean_squared_error: 1.103571, mean_q: 2.165798, mean_eps: 0.000000\n",
      " 2294/5000: episode: 450, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001306, mean_squared_error: 1.193259, mean_q: 2.282734, mean_eps: 0.000000\n",
      " 2298/5000: episode: 451, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000836, mean_squared_error: 1.183638, mean_q: 2.276209, mean_eps: 0.000000\n",
      " 2302/5000: episode: 452, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001267, mean_squared_error: 1.152558, mean_q: 2.239299, mean_eps: 0.000000\n",
      " 2306/5000: episode: 453, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000497, mean_squared_error: 1.197134, mean_q: 2.288759, mean_eps: 0.000000\n",
      " 2310/5000: episode: 454, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000967, mean_squared_error: 1.174553, mean_q: 2.274165, mean_eps: 0.000000\n",
      " 2314/5000: episode: 455, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000727, mean_squared_error: 1.093959, mean_q: 2.158236, mean_eps: 0.000000\n",
      " 2318/5000: episode: 456, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000788, mean_squared_error: 1.044274, mean_q: 2.101418, mean_eps: 0.000000\n",
      " 2322/5000: episode: 457, duration: 0.068s, episode steps: 4, steps per second: 58, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000788, mean_squared_error: 1.033244, mean_q: 2.088514, mean_eps: 0.000000\n",
      " 2326/5000: episode: 458, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000754, mean_squared_error: 1.118907, mean_q: 2.194325, mean_eps: 0.000000\n",
      " 2330/5000: episode: 459, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001195, mean_squared_error: 1.062825, mean_q: 2.123047, mean_eps: 0.000000\n",
      " 2334/5000: episode: 460, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001167, mean_squared_error: 1.046825, mean_q: 2.101416, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2338/5000: episode: 461, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000590, mean_squared_error: 1.064331, mean_q: 2.129615, mean_eps: 0.000000\n",
      " 2343/5000: episode: 462, duration: 0.072s, episode steps: 5, steps per second: 70, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.001124, mean_squared_error: 1.131921, mean_q: 2.209885, mean_eps: 0.000000\n",
      " 2347/5000: episode: 463, duration: 0.054s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000779, mean_squared_error: 1.025792, mean_q: 2.071064, mean_eps: 0.000000\n",
      " 2354/5000: episode: 464, duration: 0.091s, episode steps: 7, steps per second: 77, episode reward: 3.700, mean reward: 0.529 [-0.100, 1.000], mean action: 1.571 [1.000, 3.000], mean observation: 0.151 [0.000, 1.000], loss: 0.000904, mean_squared_error: 1.117845, mean_q: 2.192608, mean_eps: 0.000000\n",
      " 2362/5000: episode: 465, duration: 0.101s, episode steps: 8, steps per second: 79, episode reward: 3.600, mean reward: 0.450 [-0.100, 1.000], mean action: 1.500 [1.000, 3.000], mean observation: 0.153 [0.000, 1.000], loss: 0.000848, mean_squared_error: 1.153657, mean_q: 2.243732, mean_eps: 0.000000\n",
      " 2366/5000: episode: 466, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001261, mean_squared_error: 1.119063, mean_q: 2.195731, mean_eps: 0.000000\n",
      " 2370/5000: episode: 467, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000431, mean_squared_error: 1.050168, mean_q: 2.107653, mean_eps: 0.000000\n",
      " 2376/5000: episode: 468, duration: 0.081s, episode steps: 6, steps per second: 74, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 1.667 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.001257, mean_squared_error: 1.104742, mean_q: 2.185582, mean_eps: 0.000000\n",
      " 2380/5000: episode: 469, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000802, mean_squared_error: 1.117568, mean_q: 2.195125, mean_eps: 0.000000\n",
      " 2386/5000: episode: 470, duration: 0.084s, episode steps: 6, steps per second: 71, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 1.667 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.001255, mean_squared_error: 1.019888, mean_q: 2.075462, mean_eps: 0.000000\n",
      " 2391/5000: episode: 471, duration: 0.070s, episode steps: 5, steps per second: 71, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000896, mean_squared_error: 1.060278, mean_q: 2.121040, mean_eps: 0.000000\n",
      " 2395/5000: episode: 472, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000570, mean_squared_error: 1.120906, mean_q: 2.211720, mean_eps: 0.000000\n",
      " 2402/5000: episode: 473, duration: 0.097s, episode steps: 7, steps per second: 72, episode reward: 3.700, mean reward: 0.529 [-0.100, 1.000], mean action: 1.571 [1.000, 3.000], mean observation: 0.151 [0.000, 1.000], loss: 0.000944, mean_squared_error: 1.017599, mean_q: 2.067411, mean_eps: 0.000000\n",
      " 2409/5000: episode: 474, duration: 0.099s, episode steps: 7, steps per second: 71, episode reward: 3.700, mean reward: 0.529 [-0.100, 1.000], mean action: 1.571 [1.000, 3.000], mean observation: 0.151 [0.000, 1.000], loss: 0.000774, mean_squared_error: 1.041420, mean_q: 2.105201, mean_eps: 0.000000\n",
      " 2414/5000: episode: 475, duration: 0.068s, episode steps: 5, steps per second: 73, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000460, mean_squared_error: 1.104941, mean_q: 2.166348, mean_eps: 0.000000\n",
      " 2419/5000: episode: 476, duration: 0.074s, episode steps: 5, steps per second: 67, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000853, mean_squared_error: 1.018639, mean_q: 2.063404, mean_eps: 0.000000\n",
      " 2424/5000: episode: 477, duration: 0.068s, episode steps: 5, steps per second: 73, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000481, mean_squared_error: 1.125129, mean_q: 2.208455, mean_eps: 0.000000\n",
      " 2432/5000: episode: 478, duration: 0.108s, episode steps: 8, steps per second: 74, episode reward: 3.600, mean reward: 0.450 [-0.100, 1.000], mean action: 1.500 [1.000, 3.000], mean observation: 0.153 [0.000, 1.000], loss: 0.000722, mean_squared_error: 1.036432, mean_q: 2.089533, mean_eps: 0.000000\n",
      " 2437/5000: episode: 479, duration: 0.081s, episode steps: 5, steps per second: 62, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000756, mean_squared_error: 1.091691, mean_q: 2.149332, mean_eps: 0.000000\n",
      " 2441/5000: episode: 480, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000739, mean_squared_error: 1.127100, mean_q: 2.215239, mean_eps: 0.000000\n",
      " 2446/5000: episode: 481, duration: 0.074s, episode steps: 5, steps per second: 67, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000716, mean_squared_error: 1.150831, mean_q: 2.239915, mean_eps: 0.000000\n",
      " 2450/5000: episode: 482, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000655, mean_squared_error: 1.116788, mean_q: 2.187209, mean_eps: 0.000000\n",
      " 2454/5000: episode: 483, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000395, mean_squared_error: 1.113610, mean_q: 2.187162, mean_eps: 0.000000\n",
      " 2458/5000: episode: 484, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000956, mean_squared_error: 1.000035, mean_q: 2.031496, mean_eps: 0.000000\n",
      " 2464/5000: episode: 485, duration: 0.077s, episode steps: 6, steps per second: 78, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 1.667 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.000469, mean_squared_error: 1.041475, mean_q: 2.097170, mean_eps: 0.000000\n",
      " 2468/5000: episode: 486, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000647, mean_squared_error: 1.120645, mean_q: 2.187219, mean_eps: 0.000000\n",
      " 2474/5000: episode: 487, duration: 0.085s, episode steps: 6, steps per second: 70, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 1.667 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.000636, mean_squared_error: 1.074381, mean_q: 2.140353, mean_eps: 0.000000\n",
      " 2478/5000: episode: 488, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000511, mean_squared_error: 1.137410, mean_q: 2.235623, mean_eps: 0.000000\n",
      " 2483/5000: episode: 489, duration: 0.068s, episode steps: 5, steps per second: 73, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000615, mean_squared_error: 1.094097, mean_q: 2.168150, mean_eps: 0.000000\n",
      " 2487/5000: episode: 490, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000665, mean_squared_error: 1.042251, mean_q: 2.083528, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2491/5000: episode: 491, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000532, mean_squared_error: 1.053659, mean_q: 2.104646, mean_eps: 0.000000\n",
      " 2496/5000: episode: 492, duration: 0.068s, episode steps: 5, steps per second: 74, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000560, mean_squared_error: 1.079757, mean_q: 2.141097, mean_eps: 0.000000\n",
      " 2501/5000: episode: 493, duration: 0.070s, episode steps: 5, steps per second: 72, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000700, mean_squared_error: 1.023378, mean_q: 2.075700, mean_eps: 0.000000\n",
      " 2505/5000: episode: 494, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000272, mean_squared_error: 1.059399, mean_q: 2.101544, mean_eps: 0.000000\n",
      " 2509/5000: episode: 495, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000549, mean_squared_error: 1.106353, mean_q: 2.155969, mean_eps: 0.000000\n",
      " 2513/5000: episode: 496, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000127, mean_squared_error: 1.031597, mean_q: 2.070361, mean_eps: 0.000000\n",
      " 2517/5000: episode: 497, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000168, mean_squared_error: 1.018789, mean_q: 2.053686, mean_eps: 0.000000\n",
      " 2521/5000: episode: 498, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000532, mean_squared_error: 1.096650, mean_q: 2.178516, mean_eps: 0.000000\n",
      " 2525/5000: episode: 499, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000500, mean_squared_error: 1.054277, mean_q: 2.108395, mean_eps: 0.000000\n",
      " 2529/5000: episode: 500, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000259, mean_squared_error: 1.139225, mean_q: 2.217403, mean_eps: 0.000000\n",
      " 2534/5000: episode: 501, duration: 0.074s, episode steps: 5, steps per second: 67, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000457, mean_squared_error: 1.111188, mean_q: 2.190141, mean_eps: 0.000000\n",
      " 2538/5000: episode: 502, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000479, mean_squared_error: 1.033222, mean_q: 2.085713, mean_eps: 0.000000\n",
      " 2543/5000: episode: 503, duration: 0.070s, episode steps: 5, steps per second: 72, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000419, mean_squared_error: 1.048366, mean_q: 2.105507, mean_eps: 0.000000\n",
      " 2547/5000: episode: 504, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000325, mean_squared_error: 1.089240, mean_q: 2.149825, mean_eps: 0.000000\n",
      " 2551/5000: episode: 505, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000703, mean_squared_error: 1.120992, mean_q: 2.207067, mean_eps: 0.000000\n",
      " 2556/5000: episode: 506, duration: 0.064s, episode steps: 5, steps per second: 78, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000382, mean_squared_error: 1.053868, mean_q: 2.111723, mean_eps: 0.000000\n",
      " 2561/5000: episode: 507, duration: 0.071s, episode steps: 5, steps per second: 70, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000398, mean_squared_error: 1.051614, mean_q: 2.104266, mean_eps: 0.000000\n",
      " 2565/5000: episode: 508, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000103, mean_squared_error: 1.022122, mean_q: 2.074708, mean_eps: 0.000000\n",
      " 2569/5000: episode: 509, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000091, mean_squared_error: 1.125960, mean_q: 2.199158, mean_eps: 0.000000\n",
      " 2573/5000: episode: 510, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000489, mean_squared_error: 1.133293, mean_q: 2.219038, mean_eps: 0.000000\n",
      " 2578/5000: episode: 511, duration: 0.071s, episode steps: 5, steps per second: 70, episode reward: 3.900, mean reward: 0.780 [-0.100, 1.000], mean action: 1.800 [1.000, 3.000], mean observation: 0.144 [0.000, 1.000], loss: 0.000311, mean_squared_error: 1.118610, mean_q: 2.193284, mean_eps: 0.000000\n",
      " 2582/5000: episode: 512, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000054, mean_squared_error: 1.063073, mean_q: 2.116103, mean_eps: 0.000000\n",
      " 2586/5000: episode: 513, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000106, mean_squared_error: 1.035561, mean_q: 2.083898, mean_eps: 0.000000\n",
      " 2590/5000: episode: 514, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000491, mean_squared_error: 1.108873, mean_q: 2.183982, mean_eps: 0.000000\n",
      " 2594/5000: episode: 515, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000508, mean_squared_error: 1.154782, mean_q: 2.232374, mean_eps: 0.000000\n",
      " 2598/5000: episode: 516, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000063, mean_squared_error: 0.957275, mean_q: 1.969598, mean_eps: 0.000000\n",
      " 2602/5000: episode: 517, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000074, mean_squared_error: 1.021287, mean_q: 2.066420, mean_eps: 0.000000\n",
      " 2606/5000: episode: 518, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000053, mean_squared_error: 1.095244, mean_q: 2.162189, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2610/5000: episode: 519, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000373, mean_squared_error: 1.071636, mean_q: 2.127422, mean_eps: 0.000000\n",
      " 2614/5000: episode: 520, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000036, mean_squared_error: 1.006879, mean_q: 2.049784, mean_eps: 0.000000\n",
      " 2618/5000: episode: 521, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000078, mean_squared_error: 1.068029, mean_q: 2.122178, mean_eps: 0.000000\n",
      " 2622/5000: episode: 522, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000030, mean_squared_error: 1.081939, mean_q: 2.143361, mean_eps: 0.000000\n",
      " 2626/5000: episode: 523, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000518, mean_squared_error: 1.085882, mean_q: 2.137960, mean_eps: 0.000000\n",
      " 2630/5000: episode: 524, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000032, mean_squared_error: 1.080840, mean_q: 2.133537, mean_eps: 0.000000\n",
      " 2634/5000: episode: 525, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000042, mean_squared_error: 1.083686, mean_q: 2.139904, mean_eps: 0.000000\n",
      " 2638/5000: episode: 526, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000028, mean_squared_error: 1.029241, mean_q: 2.064381, mean_eps: 0.000000\n",
      " 2642/5000: episode: 527, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000649, mean_squared_error: 1.056888, mean_q: 2.108549, mean_eps: 0.000000\n",
      " 2646/5000: episode: 528, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000033, mean_squared_error: 1.124688, mean_q: 2.197386, mean_eps: 0.000000\n",
      " 2650/5000: episode: 529, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000020, mean_squared_error: 1.062894, mean_q: 2.116527, mean_eps: 0.000000\n",
      " 2654/5000: episode: 530, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000020, mean_squared_error: 0.955939, mean_q: 1.980991, mean_eps: 0.000000\n",
      " 2658/5000: episode: 531, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000539, mean_squared_error: 1.092980, mean_q: 2.166182, mean_eps: 0.000000\n",
      " 2662/5000: episode: 532, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000017, mean_squared_error: 1.187659, mean_q: 2.281869, mean_eps: 0.000000\n",
      " 2666/5000: episode: 533, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000017, mean_squared_error: 1.064793, mean_q: 2.128399, mean_eps: 0.000000\n",
      " 2670/5000: episode: 534, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000656, mean_squared_error: 1.088692, mean_q: 2.148166, mean_eps: 0.000000\n",
      " 2674/5000: episode: 535, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000022, mean_squared_error: 1.069639, mean_q: 2.141828, mean_eps: 0.000000\n",
      " 2678/5000: episode: 536, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000015, mean_squared_error: 1.047675, mean_q: 2.092331, mean_eps: 0.000000\n",
      " 2682/5000: episode: 537, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000012, mean_squared_error: 1.002176, mean_q: 2.039168, mean_eps: 0.000000\n",
      " 2686/5000: episode: 538, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000010, mean_squared_error: 1.043399, mean_q: 2.095490, mean_eps: 0.000000\n",
      " 2690/5000: episode: 539, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000010, mean_squared_error: 1.019856, mean_q: 2.055261, mean_eps: 0.000000\n",
      " 2694/5000: episode: 540, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000008, mean_squared_error: 1.019348, mean_q: 2.062300, mean_eps: 0.000000\n",
      " 2698/5000: episode: 541, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000008, mean_squared_error: 0.964180, mean_q: 1.974274, mean_eps: 0.000000\n",
      " 2702/5000: episode: 542, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000006, mean_squared_error: 1.104371, mean_q: 2.161600, mean_eps: 0.000000\n",
      " 2706/5000: episode: 543, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000006, mean_squared_error: 1.123234, mean_q: 2.211546, mean_eps: 0.000000\n",
      " 2710/5000: episode: 544, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000006, mean_squared_error: 1.060742, mean_q: 2.111611, mean_eps: 0.000000\n",
      " 2714/5000: episode: 545, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000005, mean_squared_error: 1.040284, mean_q: 2.089162, mean_eps: 0.000000\n",
      " 2718/5000: episode: 546, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000004, mean_squared_error: 1.026176, mean_q: 2.070102, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2722/5000: episode: 547, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000004, mean_squared_error: 1.155828, mean_q: 2.244553, mean_eps: 0.000000\n",
      " 2726/5000: episode: 548, duration: 0.065s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000004, mean_squared_error: 1.055694, mean_q: 2.101447, mean_eps: 0.000000\n",
      " 2730/5000: episode: 549, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000003, mean_squared_error: 1.091684, mean_q: 2.156557, mean_eps: 0.000000\n",
      " 2734/5000: episode: 550, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000004, mean_squared_error: 1.072711, mean_q: 2.133799, mean_eps: 0.000000\n",
      " 2738/5000: episode: 551, duration: 0.073s, episode steps: 4, steps per second: 55, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000003, mean_squared_error: 1.155109, mean_q: 2.237501, mean_eps: 0.000000\n",
      " 2742/5000: episode: 552, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.036582, mean_q: 2.080750, mean_eps: 0.000000\n",
      " 2746/5000: episode: 553, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.135337, mean_q: 2.217268, mean_eps: 0.000000\n",
      " 2750/5000: episode: 554, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.069343, mean_q: 2.127316, mean_eps: 0.000000\n",
      " 2754/5000: episode: 555, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.123599, mean_q: 2.192733, mean_eps: 0.000000\n",
      " 2758/5000: episode: 556, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.109847, mean_q: 2.187850, mean_eps: 0.000000\n",
      " 2762/5000: episode: 557, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 0.986807, mean_q: 2.006825, mean_eps: 0.000000\n",
      " 2766/5000: episode: 558, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.080067, mean_q: 2.138667, mean_eps: 0.000000\n",
      " 2770/5000: episode: 559, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.073023, mean_q: 2.129416, mean_eps: 0.000000\n",
      " 2774/5000: episode: 560, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.201529, mean_q: 2.312448, mean_eps: 0.000000\n",
      " 2778/5000: episode: 561, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.013233, mean_q: 2.041784, mean_eps: 0.000000\n",
      " 2782/5000: episode: 562, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.075105, mean_q: 2.135822, mean_eps: 0.000000\n",
      " 2786/5000: episode: 563, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.098018, mean_q: 2.166575, mean_eps: 0.000000\n",
      " 2790/5000: episode: 564, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.084167, mean_q: 2.150768, mean_eps: 0.000000\n",
      " 2794/5000: episode: 565, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.177529, mean_q: 2.286109, mean_eps: 0.000000\n",
      " 2798/5000: episode: 566, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.152067, mean_q: 2.236457, mean_eps: 0.000000\n",
      " 2802/5000: episode: 567, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.102501, mean_q: 2.161201, mean_eps: 0.000000\n",
      " 2806/5000: episode: 568, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.103423, mean_q: 2.180698, mean_eps: 0.000000\n",
      " 2810/5000: episode: 569, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.050134, mean_q: 2.102605, mean_eps: 0.000000\n",
      " 2814/5000: episode: 570, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.066827, mean_q: 2.126129, mean_eps: 0.000000\n",
      " 2818/5000: episode: 571, duration: 0.065s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.178610, mean_q: 2.279697, mean_eps: 0.000000\n",
      " 2822/5000: episode: 572, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.068223, mean_q: 2.125997, mean_eps: 0.000000\n",
      " 2826/5000: episode: 573, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 0.995981, mean_q: 2.035861, mean_eps: 0.000000\n",
      " 2830/5000: episode: 574, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.074276, mean_q: 2.133896, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2834/5000: episode: 575, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.085653, mean_q: 2.143440, mean_eps: 0.000000\n",
      " 2838/5000: episode: 576, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 0.973750, mean_q: 2.002936, mean_eps: 0.000000\n",
      " 2842/5000: episode: 577, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.078545, mean_q: 2.134098, mean_eps: 0.000000\n",
      " 2846/5000: episode: 578, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.033728, mean_q: 2.083306, mean_eps: 0.000000\n",
      " 2850/5000: episode: 579, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.041134, mean_q: 2.085555, mean_eps: 0.000000\n",
      " 2854/5000: episode: 580, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.175058, mean_q: 2.279552, mean_eps: 0.000000\n",
      " 2858/5000: episode: 581, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.072193, mean_q: 2.141077, mean_eps: 0.000000\n",
      " 2862/5000: episode: 582, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.053981, mean_q: 2.104837, mean_eps: 0.000000\n",
      " 2866/5000: episode: 583, duration: 0.067s, episode steps: 4, steps per second: 60, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.107577, mean_q: 2.172789, mean_eps: 0.000000\n",
      " 2870/5000: episode: 584, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.110723, mean_q: 2.172591, mean_eps: 0.000000\n",
      " 2874/5000: episode: 585, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 0.919998, mean_q: 1.916182, mean_eps: 0.000000\n",
      " 2878/5000: episode: 586, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.105313, mean_q: 2.176679, mean_eps: 0.000000\n",
      " 2882/5000: episode: 587, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.068233, mean_q: 2.123322, mean_eps: 0.000000\n",
      " 2886/5000: episode: 588, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.048660, mean_q: 2.093324, mean_eps: 0.000000\n",
      " 2890/5000: episode: 589, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.028462, mean_q: 2.066622, mean_eps: 0.000000\n",
      " 2894/5000: episode: 590, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 0.961970, mean_q: 1.979485, mean_eps: 0.000000\n",
      " 2898/5000: episode: 591, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.101873, mean_q: 2.171275, mean_eps: 0.000000\n",
      " 2902/5000: episode: 592, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.081097, mean_q: 2.142788, mean_eps: 0.000000\n",
      " 2906/5000: episode: 593, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.063595, mean_q: 2.113791, mean_eps: 0.000000\n",
      " 2910/5000: episode: 594, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.034982, mean_q: 2.082447, mean_eps: 0.000000\n",
      " 2914/5000: episode: 595, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.105978, mean_q: 2.184151, mean_eps: 0.000000\n",
      " 2918/5000: episode: 596, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.097151, mean_q: 2.161829, mean_eps: 0.000000\n",
      " 2922/5000: episode: 597, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.085047, mean_q: 2.144797, mean_eps: 0.000000\n",
      " 2926/5000: episode: 598, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.050324, mean_q: 2.104563, mean_eps: 0.000000\n",
      " 2930/5000: episode: 599, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.019057, mean_q: 2.064654, mean_eps: 0.000000\n",
      " 2934/5000: episode: 600, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.135185, mean_q: 2.201910, mean_eps: 0.000000\n",
      " 2938/5000: episode: 601, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.122683, mean_q: 2.210705, mean_eps: 0.000000\n",
      " 2942/5000: episode: 602, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.113121, mean_q: 2.196634, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2946/5000: episode: 603, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.143205, mean_q: 2.232608, mean_eps: 0.000000\n",
      " 2950/5000: episode: 604, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.074940, mean_q: 2.134049, mean_eps: 0.000000\n",
      " 2954/5000: episode: 605, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.093506, mean_q: 2.155911, mean_eps: 0.000000\n",
      " 2958/5000: episode: 606, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.035548, mean_q: 2.089713, mean_eps: 0.000000\n",
      " 2962/5000: episode: 607, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.099181, mean_q: 2.162991, mean_eps: 0.000000\n",
      " 2966/5000: episode: 608, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.037785, mean_q: 2.080609, mean_eps: 0.000000\n",
      " 2970/5000: episode: 609, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 0.999615, mean_q: 2.024375, mean_eps: 0.000000\n",
      " 2974/5000: episode: 610, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.125538, mean_q: 2.198444, mean_eps: 0.000000\n",
      " 2978/5000: episode: 611, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.065952, mean_q: 2.111608, mean_eps: 0.000000\n",
      " 2982/5000: episode: 612, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.106646, mean_q: 2.175828, mean_eps: 0.000000\n",
      " 2986/5000: episode: 613, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.130177, mean_q: 2.207909, mean_eps: 0.000000\n",
      " 2990/5000: episode: 614, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.069192, mean_q: 2.127227, mean_eps: 0.000000\n",
      " 2994/5000: episode: 615, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.118050, mean_q: 2.196051, mean_eps: 0.000000\n",
      " 2998/5000: episode: 616, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.098993, mean_q: 2.173428, mean_eps: 0.000000\n",
      " 3002/5000: episode: 617, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.017780, mean_squared_error: 1.132315, mean_q: 2.202561, mean_eps: 0.000000\n",
      " 3013/5000: episode: 618, duration: 0.139s, episode steps: 11, steps per second: 79, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 1.182 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.058802, mean_squared_error: 1.362275, mean_q: 2.415120, mean_eps: 0.000000\n",
      " 3024/5000: episode: 619, duration: 0.143s, episode steps: 11, steps per second: 77, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 1.182 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.040199, mean_squared_error: 1.312987, mean_q: 2.368856, mean_eps: 0.000000\n",
      " 3035/5000: episode: 620, duration: 0.127s, episode steps: 11, steps per second: 87, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 1.182 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.032467, mean_squared_error: 1.405024, mean_q: 2.478573, mean_eps: 0.000000\n",
      " 3046/5000: episode: 621, duration: 0.137s, episode steps: 11, steps per second: 81, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 1.182 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.022003, mean_squared_error: 1.277389, mean_q: 2.324259, mean_eps: 0.000000\n",
      " 3057/5000: episode: 622, duration: 0.145s, episode steps: 11, steps per second: 76, episode reward: 2.200, mean reward: 0.200 [-0.100, 1.000], mean action: 1.182 [1.000, 3.000], mean observation: 0.167 [0.000, 1.000], loss: 0.022185, mean_squared_error: 1.336016, mean_q: 2.391215, mean_eps: 0.000000\n",
      " 3063/5000: episode: 623, duration: 0.084s, episode steps: 6, steps per second: 71, episode reward: 3.800, mean reward: 0.633 [-0.100, 1.000], mean action: 1.667 [1.000, 3.000], mean observation: 0.148 [0.000, 1.000], loss: 0.017881, mean_squared_error: 1.312502, mean_q: 2.350866, mean_eps: 0.000000\n",
      " 3067/5000: episode: 624, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.018879, mean_squared_error: 1.094694, mean_q: 2.079016, mean_eps: 0.000000\n",
      " 3071/5000: episode: 625, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.012305, mean_squared_error: 1.328880, mean_q: 2.366233, mean_eps: 0.000000\n",
      " 3075/5000: episode: 626, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.011728, mean_squared_error: 1.320214, mean_q: 2.361386, mean_eps: 0.000000\n",
      " 3079/5000: episode: 627, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010459, mean_squared_error: 1.233893, mean_q: 2.250432, mean_eps: 0.000000\n",
      " 3083/5000: episode: 628, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004563, mean_squared_error: 1.169844, mean_q: 2.172276, mean_eps: 0.000000\n",
      " 3087/5000: episode: 629, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.008791, mean_squared_error: 1.273074, mean_q: 2.300085, mean_eps: 0.000000\n",
      " 3091/5000: episode: 630, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.012781, mean_squared_error: 1.322228, mean_q: 2.359402, mean_eps: 0.000000\n",
      " 3095/5000: episode: 631, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.016327, mean_squared_error: 1.424512, mean_q: 2.482377, mean_eps: 0.000000\n",
      " 3099/5000: episode: 632, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010081, mean_squared_error: 1.094704, mean_q: 2.076139, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3103/5000: episode: 633, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002284, mean_squared_error: 1.249801, mean_q: 2.275478, mean_eps: 0.000000\n",
      " 3107/5000: episode: 634, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.013883, mean_squared_error: 1.282969, mean_q: 2.306286, mean_eps: 0.000000\n",
      " 3111/5000: episode: 635, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001460, mean_squared_error: 1.250518, mean_q: 2.275915, mean_eps: 0.000000\n",
      " 3115/5000: episode: 636, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001194, mean_squared_error: 1.358639, mean_q: 2.406087, mean_eps: 0.000000\n",
      " 3119/5000: episode: 637, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001143, mean_squared_error: 1.487955, mean_q: 2.572488, mean_eps: 0.000000\n",
      " 3123/5000: episode: 638, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.008701, mean_squared_error: 1.222923, mean_q: 2.236445, mean_eps: 0.000000\n",
      " 3127/5000: episode: 639, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000867, mean_squared_error: 1.464297, mean_q: 2.542836, mean_eps: 0.000000\n",
      " 3131/5000: episode: 640, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004497, mean_squared_error: 1.358662, mean_q: 2.406047, mean_eps: 0.000000\n",
      " 3135/5000: episode: 641, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004276, mean_squared_error: 1.226317, mean_q: 2.241035, mean_eps: 0.000000\n",
      " 3139/5000: episode: 642, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004516, mean_squared_error: 1.266512, mean_q: 2.298298, mean_eps: 0.000000\n",
      " 3143/5000: episode: 643, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004297, mean_squared_error: 1.389023, mean_q: 2.443622, mean_eps: 0.000000\n",
      " 3147/5000: episode: 644, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000472, mean_squared_error: 1.235847, mean_q: 2.259861, mean_eps: 0.000000\n",
      " 3151/5000: episode: 645, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000442, mean_squared_error: 1.358750, mean_q: 2.410876, mean_eps: 0.000000\n",
      " 3155/5000: episode: 646, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004081, mean_squared_error: 1.175201, mean_q: 2.185981, mean_eps: 0.000000\n",
      " 3159/5000: episode: 647, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000235, mean_squared_error: 1.313860, mean_q: 2.364427, mean_eps: 0.000000\n",
      " 3163/5000: episode: 648, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000145, mean_squared_error: 1.165892, mean_q: 2.177118, mean_eps: 0.000000\n",
      " 3167/5000: episode: 649, duration: 0.068s, episode steps: 4, steps per second: 59, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000153, mean_squared_error: 1.242183, mean_q: 2.270512, mean_eps: 0.000000\n",
      " 3171/5000: episode: 650, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000139, mean_squared_error: 1.247967, mean_q: 2.280905, mean_eps: 0.000000\n",
      " 3175/5000: episode: 651, duration: 0.067s, episode steps: 4, steps per second: 60, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000350, mean_squared_error: 1.224987, mean_q: 2.245405, mean_eps: 0.000000\n",
      " 3179/5000: episode: 652, duration: 0.068s, episode steps: 4, steps per second: 59, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004236, mean_squared_error: 1.167649, mean_q: 2.172917, mean_eps: 0.000000\n",
      " 3183/5000: episode: 653, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004179, mean_squared_error: 1.275598, mean_q: 2.307841, mean_eps: 0.000000\n",
      " 3187/5000: episode: 654, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000352, mean_squared_error: 1.212839, mean_q: 2.237703, mean_eps: 0.000000\n",
      " 3191/5000: episode: 655, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007611, mean_squared_error: 1.364967, mean_q: 2.428918, mean_eps: 0.000000\n",
      " 3195/5000: episode: 656, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004001, mean_squared_error: 1.242054, mean_q: 2.281697, mean_eps: 0.000000\n",
      " 3199/5000: episode: 657, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000334, mean_squared_error: 1.188618, mean_q: 2.206376, mean_eps: 0.000000\n",
      " 3203/5000: episode: 658, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007644, mean_squared_error: 1.251702, mean_q: 2.288082, mean_eps: 0.000000\n",
      " 3207/5000: episode: 659, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000383, mean_squared_error: 1.128274, mean_q: 2.134666, mean_eps: 0.000000\n",
      " 3211/5000: episode: 660, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003952, mean_squared_error: 1.383517, mean_q: 2.459150, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3215/5000: episode: 661, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000366, mean_squared_error: 1.310252, mean_q: 2.366133, mean_eps: 0.000000\n",
      " 3219/5000: episode: 662, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000281, mean_squared_error: 1.217654, mean_q: 2.253461, mean_eps: 0.000000\n",
      " 3223/5000: episode: 663, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000286, mean_squared_error: 1.155792, mean_q: 2.171092, mean_eps: 0.000000\n",
      " 3227/5000: episode: 664, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000302, mean_squared_error: 1.311053, mean_q: 2.371313, mean_eps: 0.000000\n",
      " 3231/5000: episode: 665, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000331, mean_squared_error: 1.202115, mean_q: 2.236651, mean_eps: 0.000000\n",
      " 3235/5000: episode: 666, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000337, mean_squared_error: 1.264914, mean_q: 2.304690, mean_eps: 0.000000\n",
      " 3239/5000: episode: 667, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000224, mean_squared_error: 1.234966, mean_q: 2.277188, mean_eps: 0.000000\n",
      " 3243/5000: episode: 668, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000437, mean_squared_error: 1.396257, mean_q: 2.465688, mean_eps: 0.000000\n",
      " 3247/5000: episode: 669, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003967, mean_squared_error: 1.259459, mean_q: 2.304874, mean_eps: 0.000000\n",
      " 3251/5000: episode: 670, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.004054, mean_squared_error: 1.476314, mean_q: 2.570701, mean_eps: 0.000000\n",
      " 3255/5000: episode: 671, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000342, mean_squared_error: 1.241108, mean_q: 2.285008, mean_eps: 0.000000\n",
      " 3259/5000: episode: 672, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003960, mean_squared_error: 1.381076, mean_q: 2.455798, mean_eps: 0.000000\n",
      " 3263/5000: episode: 673, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003784, mean_squared_error: 1.303614, mean_q: 2.364711, mean_eps: 0.000000\n",
      " 3267/5000: episode: 674, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000288, mean_squared_error: 1.364644, mean_q: 2.447338, mean_eps: 0.000000\n",
      " 3271/5000: episode: 675, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003924, mean_squared_error: 1.285400, mean_q: 2.336401, mean_eps: 0.000000\n",
      " 3275/5000: episode: 676, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000381, mean_squared_error: 1.288794, mean_q: 2.342509, mean_eps: 0.000000\n",
      " 3279/5000: episode: 677, duration: 0.067s, episode steps: 4, steps per second: 59, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007411, mean_squared_error: 1.286734, mean_q: 2.342543, mean_eps: 0.000000\n",
      " 3283/5000: episode: 678, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007188, mean_squared_error: 1.231257, mean_q: 2.271844, mean_eps: 0.000000\n",
      " 3287/5000: episode: 679, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000401, mean_squared_error: 1.385406, mean_q: 2.456361, mean_eps: 0.000000\n",
      " 3291/5000: episode: 680, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003947, mean_squared_error: 1.315807, mean_q: 2.367225, mean_eps: 0.000000\n",
      " 3295/5000: episode: 681, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010292, mean_squared_error: 1.233344, mean_q: 2.270109, mean_eps: 0.000000\n",
      " 3299/5000: episode: 682, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000450, mean_squared_error: 1.171082, mean_q: 2.194944, mean_eps: 0.000000\n",
      " 3303/5000: episode: 683, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000431, mean_squared_error: 1.215208, mean_q: 2.242701, mean_eps: 0.000000\n",
      " 3307/5000: episode: 684, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007377, mean_squared_error: 1.334547, mean_q: 2.391466, mean_eps: 0.000000\n",
      " 3311/5000: episode: 685, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000265, mean_squared_error: 1.290869, mean_q: 2.360879, mean_eps: 0.000000\n",
      " 3315/5000: episode: 686, duration: 0.053s, episode steps: 4, steps per second: 76, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000426, mean_squared_error: 1.383545, mean_q: 2.469807, mean_eps: 0.000000\n",
      " 3319/5000: episode: 687, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003862, mean_squared_error: 1.410224, mean_q: 2.497467, mean_eps: 0.000000\n",
      " 3323/5000: episode: 688, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000280, mean_squared_error: 1.301260, mean_q: 2.368384, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3327/5000: episode: 689, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000403, mean_squared_error: 1.352519, mean_q: 2.428323, mean_eps: 0.000000\n",
      " 3331/5000: episode: 690, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003693, mean_squared_error: 1.491364, mean_q: 2.603902, mean_eps: 0.000000\n",
      " 3335/5000: episode: 691, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003724, mean_squared_error: 1.338987, mean_q: 2.415300, mean_eps: 0.000000\n",
      " 3339/5000: episode: 692, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000354, mean_squared_error: 1.407149, mean_q: 2.502110, mean_eps: 0.000000\n",
      " 3343/5000: episode: 693, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000305, mean_squared_error: 1.267377, mean_q: 2.323270, mean_eps: 0.000000\n",
      " 3347/5000: episode: 694, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003779, mean_squared_error: 1.253596, mean_q: 2.294413, mean_eps: 0.000000\n",
      " 3351/5000: episode: 695, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000303, mean_squared_error: 1.336871, mean_q: 2.413520, mean_eps: 0.000000\n",
      " 3355/5000: episode: 696, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003723, mean_squared_error: 1.257651, mean_q: 2.303719, mean_eps: 0.000000\n",
      " 3359/5000: episode: 697, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003652, mean_squared_error: 1.269859, mean_q: 2.323111, mean_eps: 0.000000\n",
      " 3363/5000: episode: 698, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007010, mean_squared_error: 1.342837, mean_q: 2.411564, mean_eps: 0.000000\n",
      " 3367/5000: episode: 699, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003639, mean_squared_error: 1.331196, mean_q: 2.401475, mean_eps: 0.000000\n",
      " 3371/5000: episode: 700, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000406, mean_squared_error: 1.362880, mean_q: 2.447893, mean_eps: 0.000000\n",
      " 3375/5000: episode: 701, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003722, mean_squared_error: 1.362710, mean_q: 2.439262, mean_eps: 0.000000\n",
      " 3379/5000: episode: 702, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003498, mean_squared_error: 1.250204, mean_q: 2.303849, mean_eps: 0.000000\n",
      " 3383/5000: episode: 703, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000333, mean_squared_error: 1.212090, mean_q: 2.253963, mean_eps: 0.000000\n",
      " 3387/5000: episode: 704, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000438, mean_squared_error: 1.394320, mean_q: 2.469728, mean_eps: 0.000000\n",
      " 3391/5000: episode: 705, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000441, mean_squared_error: 1.360886, mean_q: 2.438927, mean_eps: 0.000000\n",
      " 3395/5000: episode: 706, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000349, mean_squared_error: 1.277907, mean_q: 2.339456, mean_eps: 0.000000\n",
      " 3399/5000: episode: 707, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000284, mean_squared_error: 1.271092, mean_q: 2.331952, mean_eps: 0.000000\n",
      " 3403/5000: episode: 708, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003927, mean_squared_error: 1.249093, mean_q: 2.291429, mean_eps: 0.000000\n",
      " 3407/5000: episode: 709, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000344, mean_squared_error: 1.302015, mean_q: 2.361897, mean_eps: 0.000000\n",
      " 3411/5000: episode: 710, duration: 0.087s, episode steps: 4, steps per second: 46, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.010417, mean_squared_error: 1.185655, mean_q: 2.216977, mean_eps: 0.000000\n",
      " 3415/5000: episode: 711, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.007042, mean_squared_error: 1.193116, mean_q: 2.229311, mean_eps: 0.000000\n",
      " 3419/5000: episode: 712, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000339, mean_squared_error: 1.273370, mean_q: 2.330446, mean_eps: 0.000000\n",
      " 3423/5000: episode: 713, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006875, mean_squared_error: 1.358128, mean_q: 2.428358, mean_eps: 0.000000\n",
      " 3427/5000: episode: 714, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000507, mean_squared_error: 1.201213, mean_q: 2.239085, mean_eps: 0.000000\n",
      " 3431/5000: episode: 715, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003581, mean_squared_error: 1.292716, mean_q: 2.361923, mean_eps: 0.000000\n",
      " 3435/5000: episode: 716, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000404, mean_squared_error: 1.336209, mean_q: 2.417935, mean_eps: 0.000000\n",
      " 3439/5000: episode: 717, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000235, mean_squared_error: 1.324861, mean_q: 2.398183, mean_eps: 0.000000\n",
      " 3443/5000: episode: 718, duration: 0.065s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000367, mean_squared_error: 1.276525, mean_q: 2.332329, mean_eps: 0.000000\n",
      " 3447/5000: episode: 719, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000395, mean_squared_error: 1.354506, mean_q: 2.425773, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3451/5000: episode: 720, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000348, mean_squared_error: 1.342417, mean_q: 2.409869, mean_eps: 0.000000\n",
      " 3455/5000: episode: 721, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000131, mean_squared_error: 1.296111, mean_q: 2.363932, mean_eps: 0.000000\n",
      " 3459/5000: episode: 722, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000391, mean_squared_error: 1.198135, mean_q: 2.243748, mean_eps: 0.000000\n",
      " 3463/5000: episode: 723, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003713, mean_squared_error: 1.273485, mean_q: 2.320173, mean_eps: 0.000000\n",
      " 3467/5000: episode: 724, duration: 0.068s, episode steps: 4, steps per second: 59, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000454, mean_squared_error: 1.288984, mean_q: 2.348718, mean_eps: 0.000000\n",
      " 3471/5000: episode: 725, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000235, mean_squared_error: 1.395754, mean_q: 2.486573, mean_eps: 0.000000\n",
      " 3475/5000: episode: 726, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003519, mean_squared_error: 1.337686, mean_q: 2.406833, mean_eps: 0.000000\n",
      " 3479/5000: episode: 727, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003760, mean_squared_error: 1.187510, mean_q: 2.221271, mean_eps: 0.000000\n",
      " 3483/5000: episode: 728, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003512, mean_squared_error: 1.261085, mean_q: 2.317476, mean_eps: 0.000000\n",
      " 3487/5000: episode: 729, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000381, mean_squared_error: 1.338191, mean_q: 2.420052, mean_eps: 0.000000\n",
      " 3491/5000: episode: 730, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000303, mean_squared_error: 1.489951, mean_q: 2.601407, mean_eps: 0.000000\n",
      " 3495/5000: episode: 731, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000456, mean_squared_error: 1.314380, mean_q: 2.376858, mean_eps: 0.000000\n",
      " 3499/5000: episode: 732, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000354, mean_squared_error: 1.251297, mean_q: 2.305799, mean_eps: 0.000000\n",
      " 3503/5000: episode: 733, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003755, mean_squared_error: 1.434590, mean_q: 2.536639, mean_eps: 0.000000\n",
      " 3507/5000: episode: 734, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006918, mean_squared_error: 1.328163, mean_q: 2.398818, mean_eps: 0.000000\n",
      " 3511/5000: episode: 735, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003468, mean_squared_error: 1.353819, mean_q: 2.430299, mean_eps: 0.000000\n",
      " 3515/5000: episode: 736, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000404, mean_squared_error: 1.290420, mean_q: 2.356879, mean_eps: 0.000000\n",
      " 3519/5000: episode: 737, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000341, mean_squared_error: 1.379620, mean_q: 2.463570, mean_eps: 0.000000\n",
      " 3523/5000: episode: 738, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000368, mean_squared_error: 1.229103, mean_q: 2.274865, mean_eps: 0.000000\n",
      " 3527/5000: episode: 739, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000326, mean_squared_error: 1.317618, mean_q: 2.383357, mean_eps: 0.000000\n",
      " 3531/5000: episode: 740, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006829, mean_squared_error: 1.352952, mean_q: 2.433393, mean_eps: 0.000000\n",
      " 3535/5000: episode: 741, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003486, mean_squared_error: 1.262817, mean_q: 2.318336, mean_eps: 0.000000\n",
      " 3539/5000: episode: 742, duration: 0.052s, episode steps: 4, steps per second: 76, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000268, mean_squared_error: 1.261986, mean_q: 2.320529, mean_eps: 0.000000\n",
      " 3543/5000: episode: 743, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000310, mean_squared_error: 1.238702, mean_q: 2.284034, mean_eps: 0.000000\n",
      " 3547/5000: episode: 744, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003635, mean_squared_error: 1.117319, mean_q: 2.130724, mean_eps: 0.000000\n",
      " 3551/5000: episode: 745, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000454, mean_squared_error: 1.272575, mean_q: 2.333606, mean_eps: 0.000000\n",
      " 3555/5000: episode: 746, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006822, mean_squared_error: 1.234648, mean_q: 2.281667, mean_eps: 0.000000\n",
      " 3559/5000: episode: 747, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000504, mean_squared_error: 1.240395, mean_q: 2.294663, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3563/5000: episode: 748, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000262, mean_squared_error: 1.380968, mean_q: 2.459913, mean_eps: 0.000000\n",
      " 3567/5000: episode: 749, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003521, mean_squared_error: 1.284376, mean_q: 2.352507, mean_eps: 0.000000\n",
      " 3571/5000: episode: 750, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000446, mean_squared_error: 1.162324, mean_q: 2.193696, mean_eps: 0.000000\n",
      " 3575/5000: episode: 751, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000477, mean_squared_error: 1.316363, mean_q: 2.391830, mean_eps: 0.000000\n",
      " 3579/5000: episode: 752, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000468, mean_squared_error: 1.344375, mean_q: 2.418509, mean_eps: 0.000000\n",
      " 3583/5000: episode: 753, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000293, mean_squared_error: 1.256384, mean_q: 2.311491, mean_eps: 0.000000\n",
      " 3587/5000: episode: 754, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000422, mean_squared_error: 1.253044, mean_q: 2.309143, mean_eps: 0.000000\n",
      " 3591/5000: episode: 755, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000347, mean_squared_error: 1.181043, mean_q: 2.224054, mean_eps: 0.000000\n",
      " 3595/5000: episode: 756, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000470, mean_squared_error: 1.336146, mean_q: 2.407013, mean_eps: 0.000000\n",
      " 3599/5000: episode: 757, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000518, mean_squared_error: 1.324873, mean_q: 2.387606, mean_eps: 0.000000\n",
      " 3603/5000: episode: 758, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003474, mean_squared_error: 1.360577, mean_q: 2.437110, mean_eps: 0.000000\n",
      " 3607/5000: episode: 759, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000495, mean_squared_error: 1.378371, mean_q: 2.461657, mean_eps: 0.000000\n",
      " 3611/5000: episode: 760, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006465, mean_squared_error: 1.328773, mean_q: 2.404321, mean_eps: 0.000000\n",
      " 3615/5000: episode: 761, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003532, mean_squared_error: 1.216663, mean_q: 2.262309, mean_eps: 0.000000\n",
      " 3619/5000: episode: 762, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000439, mean_squared_error: 1.441388, mean_q: 2.536197, mean_eps: 0.000000\n",
      " 3623/5000: episode: 763, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000353, mean_squared_error: 1.309916, mean_q: 2.384447, mean_eps: 0.000000\n",
      " 3627/5000: episode: 764, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003415, mean_squared_error: 1.238987, mean_q: 2.291205, mean_eps: 0.000000\n",
      " 3631/5000: episode: 765, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003811, mean_squared_error: 1.417530, mean_q: 2.503309, mean_eps: 0.000000\n",
      " 3635/5000: episode: 766, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003437, mean_squared_error: 1.314656, mean_q: 2.388258, mean_eps: 0.000000\n",
      " 3639/5000: episode: 767, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000425, mean_squared_error: 1.357682, mean_q: 2.441992, mean_eps: 0.000000\n",
      " 3643/5000: episode: 768, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006888, mean_squared_error: 1.293028, mean_q: 2.351453, mean_eps: 0.000000\n",
      " 3647/5000: episode: 769, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006625, mean_squared_error: 1.267840, mean_q: 2.322747, mean_eps: 0.000000\n",
      " 3651/5000: episode: 770, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000361, mean_squared_error: 1.433617, mean_q: 2.536492, mean_eps: 0.000000\n",
      " 3655/5000: episode: 771, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003550, mean_squared_error: 1.283036, mean_q: 2.332882, mean_eps: 0.000000\n",
      " 3659/5000: episode: 772, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000414, mean_squared_error: 1.139195, mean_q: 2.169414, mean_eps: 0.000000\n",
      " 3663/5000: episode: 773, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006624, mean_squared_error: 1.205404, mean_q: 2.241797, mean_eps: 0.000000\n",
      " 3667/5000: episode: 774, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003475, mean_squared_error: 1.333570, mean_q: 2.418354, mean_eps: 0.000000\n",
      " 3671/5000: episode: 775, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000414, mean_squared_error: 1.428196, mean_q: 2.528067, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3675/5000: episode: 776, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006564, mean_squared_error: 1.227020, mean_q: 2.271770, mean_eps: 0.000000\n",
      " 3679/5000: episode: 777, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006245, mean_squared_error: 1.210851, mean_q: 2.257760, mean_eps: 0.000000\n",
      " 3683/5000: episode: 778, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000341, mean_squared_error: 1.297840, mean_q: 2.364631, mean_eps: 0.000000\n",
      " 3687/5000: episode: 779, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000448, mean_squared_error: 1.170001, mean_q: 2.196404, mean_eps: 0.000000\n",
      " 3691/5000: episode: 780, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003472, mean_squared_error: 1.317655, mean_q: 2.391051, mean_eps: 0.000000\n",
      " 3695/5000: episode: 781, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006496, mean_squared_error: 1.329691, mean_q: 2.401758, mean_eps: 0.000000\n",
      " 3699/5000: episode: 782, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006634, mean_squared_error: 1.211548, mean_q: 2.258040, mean_eps: 0.000000\n",
      " 3703/5000: episode: 783, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000560, mean_squared_error: 1.402470, mean_q: 2.495088, mean_eps: 0.000000\n",
      " 3707/5000: episode: 784, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000504, mean_squared_error: 1.348425, mean_q: 2.426356, mean_eps: 0.000000\n",
      " 3711/5000: episode: 785, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003604, mean_squared_error: 1.350014, mean_q: 2.424523, mean_eps: 0.000000\n",
      " 3715/5000: episode: 786, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006511, mean_squared_error: 1.292167, mean_q: 2.356250, mean_eps: 0.000000\n",
      " 3719/5000: episode: 787, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000226, mean_squared_error: 1.328449, mean_q: 2.407035, mean_eps: 0.000000\n",
      " 3723/5000: episode: 788, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.009173, mean_squared_error: 1.287348, mean_q: 2.354394, mean_eps: 0.000000\n",
      " 3727/5000: episode: 789, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000381, mean_squared_error: 1.173451, mean_q: 2.213739, mean_eps: 0.000000\n",
      " 3731/5000: episode: 790, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000446, mean_squared_error: 1.346842, mean_q: 2.421153, mean_eps: 0.000000\n",
      " 3735/5000: episode: 791, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006671, mean_squared_error: 1.373624, mean_q: 2.455558, mean_eps: 0.000000\n",
      " 3739/5000: episode: 792, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.011684, mean_squared_error: 1.336856, mean_q: 2.404779, mean_eps: 0.000000\n",
      " 3743/5000: episode: 793, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000417, mean_squared_error: 1.406629, mean_q: 2.503632, mean_eps: 0.000000\n",
      " 3747/5000: episode: 794, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003617, mean_squared_error: 1.292630, mean_q: 2.358194, mean_eps: 0.000000\n",
      " 3751/5000: episode: 795, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003410, mean_squared_error: 1.338575, mean_q: 2.412151, mean_eps: 0.000000\n",
      " 3755/5000: episode: 796, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000521, mean_squared_error: 1.298992, mean_q: 2.369927, mean_eps: 0.000000\n",
      " 3759/5000: episode: 797, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006543, mean_squared_error: 1.316615, mean_q: 2.379990, mean_eps: 0.000000\n",
      " 3763/5000: episode: 798, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003386, mean_squared_error: 1.211738, mean_q: 2.263841, mean_eps: 0.000000\n",
      " 3767/5000: episode: 799, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003213, mean_squared_error: 1.294964, mean_q: 2.362696, mean_eps: 0.000000\n",
      " 3771/5000: episode: 800, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000734, mean_squared_error: 1.204036, mean_q: 2.243161, mean_eps: 0.000000\n",
      " 3775/5000: episode: 801, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003150, mean_squared_error: 1.334036, mean_q: 2.413089, mean_eps: 0.000000\n",
      " 3779/5000: episode: 802, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000324, mean_squared_error: 1.246518, mean_q: 2.316244, mean_eps: 0.000000\n",
      " 3783/5000: episode: 803, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003849, mean_squared_error: 1.341260, mean_q: 2.419169, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3787/5000: episode: 804, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.006449, mean_squared_error: 1.277416, mean_q: 2.328376, mean_eps: 0.000000\n",
      " 3791/5000: episode: 805, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003045, mean_squared_error: 1.345349, mean_q: 2.427649, mean_eps: 0.000000\n",
      " 3795/5000: episode: 806, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000216, mean_squared_error: 1.435715, mean_q: 2.540209, mean_eps: 0.000000\n",
      " 3799/5000: episode: 807, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003373, mean_squared_error: 1.362886, mean_q: 2.446354, mean_eps: 0.000000\n",
      " 3803/5000: episode: 808, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000775, mean_squared_error: 1.355708, mean_q: 2.430877, mean_eps: 0.000000\n",
      " 3807/5000: episode: 809, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000689, mean_squared_error: 1.339379, mean_q: 2.412849, mean_eps: 0.000000\n",
      " 3811/5000: episode: 810, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000785, mean_squared_error: 1.282537, mean_q: 2.335449, mean_eps: 0.000000\n",
      " 3815/5000: episode: 811, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003408, mean_squared_error: 1.251200, mean_q: 2.315133, mean_eps: 0.000000\n",
      " 3819/5000: episode: 812, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000179, mean_squared_error: 1.259500, mean_q: 2.320493, mean_eps: 0.000000\n",
      " 3823/5000: episode: 813, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000453, mean_squared_error: 1.280815, mean_q: 2.349444, mean_eps: 0.000000\n",
      " 3827/5000: episode: 814, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000407, mean_squared_error: 1.482030, mean_q: 2.590330, mean_eps: 0.000000\n",
      " 3831/5000: episode: 815, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000633, mean_squared_error: 1.363914, mean_q: 2.442589, mean_eps: 0.000000\n",
      " 3835/5000: episode: 816, duration: 0.067s, episode steps: 4, steps per second: 60, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003196, mean_squared_error: 1.348862, mean_q: 2.433379, mean_eps: 0.000000\n",
      " 3839/5000: episode: 817, duration: 0.053s, episode steps: 4, steps per second: 76, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000104, mean_squared_error: 1.356686, mean_q: 2.442840, mean_eps: 0.000000\n",
      " 3843/5000: episode: 818, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000542, mean_squared_error: 1.392267, mean_q: 2.478265, mean_eps: 0.000000\n",
      " 3847/5000: episode: 819, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.003154, mean_squared_error: 1.316720, mean_q: 2.392147, mean_eps: 0.000000\n",
      " 3851/5000: episode: 820, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000260, mean_squared_error: 1.294389, mean_q: 2.362947, mean_eps: 0.000000\n",
      " 3855/5000: episode: 821, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000153, mean_squared_error: 1.321008, mean_q: 2.400770, mean_eps: 0.000000\n",
      " 3859/5000: episode: 822, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000050, mean_squared_error: 1.262035, mean_q: 2.331333, mean_eps: 0.000000\n",
      " 3863/5000: episode: 823, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000165, mean_squared_error: 1.382291, mean_q: 2.475934, mean_eps: 0.000000\n",
      " 3867/5000: episode: 824, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000178, mean_squared_error: 1.382597, mean_q: 2.471983, mean_eps: 0.000000\n",
      " 3871/5000: episode: 825, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000192, mean_squared_error: 1.396250, mean_q: 2.494983, mean_eps: 0.000000\n",
      " 3875/5000: episode: 826, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000303, mean_squared_error: 1.401482, mean_q: 2.487933, mean_eps: 0.000000\n",
      " 3879/5000: episode: 827, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000261, mean_squared_error: 1.354958, mean_q: 2.436460, mean_eps: 0.000000\n",
      " 3883/5000: episode: 828, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000323, mean_squared_error: 1.341918, mean_q: 2.417893, mean_eps: 0.000000\n",
      " 3887/5000: episode: 829, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000174, mean_squared_error: 1.375779, mean_q: 2.469707, mean_eps: 0.000000\n",
      " 3891/5000: episode: 830, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000211, mean_squared_error: 1.347648, mean_q: 2.431406, mean_eps: 0.000000\n",
      " 3895/5000: episode: 831, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000172, mean_squared_error: 1.351195, mean_q: 2.433440, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3899/5000: episode: 832, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000163, mean_squared_error: 1.334601, mean_q: 2.415130, mean_eps: 0.000000\n",
      " 3903/5000: episode: 833, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000264, mean_squared_error: 1.340229, mean_q: 2.416442, mean_eps: 0.000000\n",
      " 3907/5000: episode: 834, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000063, mean_squared_error: 1.311631, mean_q: 2.377500, mean_eps: 0.000000\n",
      " 3911/5000: episode: 835, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000182, mean_squared_error: 1.439830, mean_q: 2.549954, mean_eps: 0.000000\n",
      " 3915/5000: episode: 836, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000259, mean_squared_error: 1.340722, mean_q: 2.423801, mean_eps: 0.000000\n",
      " 3919/5000: episode: 837, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000037, mean_squared_error: 1.303381, mean_q: 2.380821, mean_eps: 0.000000\n",
      " 3923/5000: episode: 838, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000261, mean_squared_error: 1.188190, mean_q: 2.219818, mean_eps: 0.000000\n",
      " 3927/5000: episode: 839, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000133, mean_squared_error: 1.219278, mean_q: 2.269619, mean_eps: 0.000000\n",
      " 3931/5000: episode: 840, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000202, mean_squared_error: 1.335740, mean_q: 2.406643, mean_eps: 0.000000\n",
      " 3935/5000: episode: 841, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000276, mean_squared_error: 1.432293, mean_q: 2.528816, mean_eps: 0.000000\n",
      " 3939/5000: episode: 842, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000319, mean_squared_error: 1.430310, mean_q: 2.527858, mean_eps: 0.000000\n",
      " 3943/5000: episode: 843, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000200, mean_squared_error: 1.323714, mean_q: 2.394950, mean_eps: 0.000000\n",
      " 3947/5000: episode: 844, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000254, mean_squared_error: 1.372201, mean_q: 2.457733, mean_eps: 0.000000\n",
      " 3951/5000: episode: 845, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000319, mean_squared_error: 1.305748, mean_q: 2.369675, mean_eps: 0.000000\n",
      " 3955/5000: episode: 846, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000222, mean_squared_error: 1.307387, mean_q: 2.376333, mean_eps: 0.000000\n",
      " 3959/5000: episode: 847, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000364, mean_squared_error: 1.349077, mean_q: 2.432102, mean_eps: 0.000000\n",
      " 3963/5000: episode: 848, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000289, mean_squared_error: 1.445490, mean_q: 2.545377, mean_eps: 0.000000\n",
      " 3967/5000: episode: 849, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000207, mean_squared_error: 1.334481, mean_q: 2.418093, mean_eps: 0.000000\n",
      " 3971/5000: episode: 850, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000346, mean_squared_error: 1.441319, mean_q: 2.542367, mean_eps: 0.000000\n",
      " 3975/5000: episode: 851, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000121, mean_squared_error: 1.149659, mean_q: 2.185286, mean_eps: 0.000000\n",
      " 3979/5000: episode: 852, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000416, mean_squared_error: 1.525993, mean_q: 2.643783, mean_eps: 0.000000\n",
      " 3983/5000: episode: 853, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000307, mean_squared_error: 1.197095, mean_q: 2.232370, mean_eps: 0.000000\n",
      " 3987/5000: episode: 854, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000346, mean_squared_error: 1.324101, mean_q: 2.392266, mean_eps: 0.000000\n",
      " 3991/5000: episode: 855, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000254, mean_squared_error: 1.326888, mean_q: 2.402931, mean_eps: 0.000000\n",
      " 3995/5000: episode: 856, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000082, mean_squared_error: 1.275321, mean_q: 2.335393, mean_eps: 0.000000\n",
      " 3999/5000: episode: 857, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000228, mean_squared_error: 1.405277, mean_q: 2.503076, mean_eps: 0.000000\n",
      " 4003/5000: episode: 858, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002825, mean_squared_error: 1.401232, mean_q: 2.495998, mean_eps: 0.000000\n",
      " 4007/5000: episode: 859, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.002167, mean_squared_error: 1.332348, mean_q: 2.410206, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4011/5000: episode: 860, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001361, mean_squared_error: 1.320292, mean_q: 2.383677, mean_eps: 0.000000\n",
      " 4015/5000: episode: 861, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.001217, mean_squared_error: 1.380817, mean_q: 2.482811, mean_eps: 0.000000\n",
      " 4019/5000: episode: 862, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000820, mean_squared_error: 1.541037, mean_q: 2.654059, mean_eps: 0.000000\n",
      " 4023/5000: episode: 863, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000573, mean_squared_error: 1.438512, mean_q: 2.561253, mean_eps: 0.000000\n",
      " 4027/5000: episode: 864, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000523, mean_squared_error: 1.529290, mean_q: 2.653836, mean_eps: 0.000000\n",
      " 4031/5000: episode: 865, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000469, mean_squared_error: 1.418429, mean_q: 2.503383, mean_eps: 0.000000\n",
      " 4035/5000: episode: 866, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000459, mean_squared_error: 1.314665, mean_q: 2.401776, mean_eps: 0.000000\n",
      " 4039/5000: episode: 867, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000278, mean_squared_error: 1.364413, mean_q: 2.421849, mean_eps: 0.000000\n",
      " 4043/5000: episode: 868, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000334, mean_squared_error: 1.410107, mean_q: 2.497743, mean_eps: 0.000000\n",
      " 4047/5000: episode: 869, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000190, mean_squared_error: 1.434811, mean_q: 2.502181, mean_eps: 0.000000\n",
      " 4051/5000: episode: 870, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000362, mean_squared_error: 1.267620, mean_q: 2.308292, mean_eps: 0.000000\n",
      " 4055/5000: episode: 871, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000229, mean_squared_error: 1.555075, mean_q: 2.697619, mean_eps: 0.000000\n",
      " 4059/5000: episode: 872, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000157, mean_squared_error: 1.248129, mean_q: 2.294344, mean_eps: 0.000000\n",
      " 4063/5000: episode: 873, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000128, mean_squared_error: 1.354243, mean_q: 2.421785, mean_eps: 0.000000\n",
      " 4067/5000: episode: 874, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000195, mean_squared_error: 1.395775, mean_q: 2.467973, mean_eps: 0.000000\n",
      " 4071/5000: episode: 875, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000146, mean_squared_error: 1.468175, mean_q: 2.564875, mean_eps: 0.000000\n",
      " 4075/5000: episode: 876, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000074, mean_squared_error: 1.445737, mean_q: 2.541974, mean_eps: 0.000000\n",
      " 4079/5000: episode: 877, duration: 0.073s, episode steps: 4, steps per second: 55, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000122, mean_squared_error: 1.375182, mean_q: 2.467750, mean_eps: 0.000000\n",
      " 4083/5000: episode: 878, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000132, mean_squared_error: 1.328594, mean_q: 2.398168, mean_eps: 0.000000\n",
      " 4087/5000: episode: 879, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000120, mean_squared_error: 1.426394, mean_q: 2.495733, mean_eps: 0.000000\n",
      " 4091/5000: episode: 880, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000062, mean_squared_error: 1.273070, mean_q: 2.317281, mean_eps: 0.000000\n",
      " 4095/5000: episode: 881, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000078, mean_squared_error: 1.442179, mean_q: 2.538245, mean_eps: 0.000000\n",
      " 4099/5000: episode: 882, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000073, mean_squared_error: 1.335718, mean_q: 2.397560, mean_eps: 0.000000\n",
      " 4103/5000: episode: 883, duration: 0.055s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000102, mean_squared_error: 1.381546, mean_q: 2.449218, mean_eps: 0.000000\n",
      " 4107/5000: episode: 884, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000074, mean_squared_error: 1.415689, mean_q: 2.497581, mean_eps: 0.000000\n",
      " 4111/5000: episode: 885, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000075, mean_squared_error: 1.288978, mean_q: 2.345678, mean_eps: 0.000000\n",
      " 4115/5000: episode: 886, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000078, mean_squared_error: 1.552797, mean_q: 2.693234, mean_eps: 0.000000\n",
      " 4119/5000: episode: 887, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000072, mean_squared_error: 1.207295, mean_q: 2.240880, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4123/5000: episode: 888, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000065, mean_squared_error: 1.451288, mean_q: 2.546423, mean_eps: 0.000000\n",
      " 4127/5000: episode: 889, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000068, mean_squared_error: 1.327939, mean_q: 2.394051, mean_eps: 0.000000\n",
      " 4131/5000: episode: 890, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000064, mean_squared_error: 1.401350, mean_q: 2.477394, mean_eps: 0.000000\n",
      " 4135/5000: episode: 891, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000041, mean_squared_error: 1.510775, mean_q: 2.622663, mean_eps: 0.000000\n",
      " 4139/5000: episode: 892, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000050, mean_squared_error: 1.266843, mean_q: 2.316456, mean_eps: 0.000000\n",
      " 4143/5000: episode: 893, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000031, mean_squared_error: 1.304704, mean_q: 2.347478, mean_eps: 0.000000\n",
      " 4147/5000: episode: 894, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000033, mean_squared_error: 1.478945, mean_q: 2.584357, mean_eps: 0.000000\n",
      " 4151/5000: episode: 895, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000040, mean_squared_error: 1.403494, mean_q: 2.491991, mean_eps: 0.000000\n",
      " 4155/5000: episode: 896, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000025, mean_squared_error: 1.458958, mean_q: 2.568865, mean_eps: 0.000000\n",
      " 4159/5000: episode: 897, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000031, mean_squared_error: 1.487572, mean_q: 2.605252, mean_eps: 0.000000\n",
      " 4163/5000: episode: 898, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000027, mean_squared_error: 1.473619, mean_q: 2.584545, mean_eps: 0.000000\n",
      " 4167/5000: episode: 899, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000020, mean_squared_error: 1.425763, mean_q: 2.530291, mean_eps: 0.000000\n",
      " 4171/5000: episode: 900, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000025, mean_squared_error: 1.315245, mean_q: 2.398230, mean_eps: 0.000000\n",
      " 4175/5000: episode: 901, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000021, mean_squared_error: 1.354584, mean_q: 2.413354, mean_eps: 0.000000\n",
      " 4179/5000: episode: 902, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000025, mean_squared_error: 1.442454, mean_q: 2.521569, mean_eps: 0.000000\n",
      " 4183/5000: episode: 903, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000015, mean_squared_error: 1.425292, mean_q: 2.527318, mean_eps: 0.000000\n",
      " 4187/5000: episode: 904, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000013, mean_squared_error: 1.357563, mean_q: 2.420081, mean_eps: 0.000000\n",
      " 4191/5000: episode: 905, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000015, mean_squared_error: 1.475958, mean_q: 2.591606, mean_eps: 0.000000\n",
      " 4195/5000: episode: 906, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000014, mean_squared_error: 1.515005, mean_q: 2.643993, mean_eps: 0.000000\n",
      " 4199/5000: episode: 907, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000015, mean_squared_error: 1.323239, mean_q: 2.398827, mean_eps: 0.000000\n",
      " 4203/5000: episode: 908, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000013, mean_squared_error: 1.424752, mean_q: 2.497343, mean_eps: 0.000000\n",
      " 4207/5000: episode: 909, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000013, mean_squared_error: 1.469107, mean_q: 2.559437, mean_eps: 0.000000\n",
      " 4211/5000: episode: 910, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000013, mean_squared_error: 1.463165, mean_q: 2.559462, mean_eps: 0.000000\n",
      " 4215/5000: episode: 911, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000012, mean_squared_error: 1.278033, mean_q: 2.343201, mean_eps: 0.000000\n",
      " 4219/5000: episode: 912, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000008, mean_squared_error: 1.438087, mean_q: 2.543159, mean_eps: 0.000000\n",
      " 4223/5000: episode: 913, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000010, mean_squared_error: 1.435598, mean_q: 2.543096, mean_eps: 0.000000\n",
      " 4227/5000: episode: 914, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000007, mean_squared_error: 1.467867, mean_q: 2.574242, mean_eps: 0.000000\n",
      " 4231/5000: episode: 915, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000010, mean_squared_error: 1.396446, mean_q: 2.473844, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4235/5000: episode: 916, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000009, mean_squared_error: 1.340569, mean_q: 2.404503, mean_eps: 0.000000\n",
      " 4239/5000: episode: 917, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000007, mean_squared_error: 1.466338, mean_q: 2.589480, mean_eps: 0.000000\n",
      " 4243/5000: episode: 918, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000006, mean_squared_error: 1.329945, mean_q: 2.396354, mean_eps: 0.000000\n",
      " 4247/5000: episode: 919, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000005, mean_squared_error: 1.448287, mean_q: 2.550211, mean_eps: 0.000000\n",
      " 4251/5000: episode: 920, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000005, mean_squared_error: 1.271546, mean_q: 2.327229, mean_eps: 0.000000\n",
      " 4255/5000: episode: 921, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000005, mean_squared_error: 1.428757, mean_q: 2.535704, mean_eps: 0.000000\n",
      " 4259/5000: episode: 922, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000005, mean_squared_error: 1.323631, mean_q: 2.396154, mean_eps: 0.000000\n",
      " 4263/5000: episode: 923, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000004, mean_squared_error: 1.381833, mean_q: 2.473095, mean_eps: 0.000000\n",
      " 4267/5000: episode: 924, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000004, mean_squared_error: 1.289902, mean_q: 2.357699, mean_eps: 0.000000\n",
      " 4271/5000: episode: 925, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000004, mean_squared_error: 1.370287, mean_q: 2.418740, mean_eps: 0.000000\n",
      " 4275/5000: episode: 926, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000003, mean_squared_error: 1.533038, mean_q: 2.657546, mean_eps: 0.000000\n",
      " 4279/5000: episode: 927, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000003, mean_squared_error: 1.420841, mean_q: 2.527394, mean_eps: 0.000000\n",
      " 4283/5000: episode: 928, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000003, mean_squared_error: 1.288840, mean_q: 2.334294, mean_eps: 0.000000\n",
      " 4287/5000: episode: 929, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000003, mean_squared_error: 1.426789, mean_q: 2.503161, mean_eps: 0.000000\n",
      " 4291/5000: episode: 930, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000003, mean_squared_error: 1.496322, mean_q: 2.603781, mean_eps: 0.000000\n",
      " 4295/5000: episode: 931, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.440574, mean_q: 2.550154, mean_eps: 0.000000\n",
      " 4299/5000: episode: 932, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.368856, mean_q: 2.434072, mean_eps: 0.000000\n",
      " 4303/5000: episode: 933, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.424783, mean_q: 2.519079, mean_eps: 0.000000\n",
      " 4307/5000: episode: 934, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.398476, mean_q: 2.488202, mean_eps: 0.000000\n",
      " 4311/5000: episode: 935, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.338708, mean_q: 2.402963, mean_eps: 0.000000\n",
      " 4315/5000: episode: 936, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000002, mean_squared_error: 1.499709, mean_q: 2.611249, mean_eps: 0.000000\n",
      " 4319/5000: episode: 937, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.488382, mean_q: 2.595603, mean_eps: 0.000000\n",
      " 4323/5000: episode: 938, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.377643, mean_q: 2.464668, mean_eps: 0.000000\n",
      " 4327/5000: episode: 939, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.398004, mean_q: 2.464051, mean_eps: 0.000000\n",
      " 4331/5000: episode: 940, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.474320, mean_q: 2.572192, mean_eps: 0.000000\n",
      " 4335/5000: episode: 941, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.570048, mean_q: 2.695684, mean_eps: 0.000000\n",
      " 4339/5000: episode: 942, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.423032, mean_q: 2.511131, mean_eps: 0.000000\n",
      " 4343/5000: episode: 943, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.370517, mean_q: 2.449221, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4347/5000: episode: 944, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.329945, mean_q: 2.395169, mean_eps: 0.000000\n",
      " 4351/5000: episode: 945, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.342536, mean_q: 2.418521, mean_eps: 0.000000\n",
      " 4355/5000: episode: 946, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.382430, mean_q: 2.472516, mean_eps: 0.000000\n",
      " 4359/5000: episode: 947, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.447970, mean_q: 2.557615, mean_eps: 0.000000\n",
      " 4363/5000: episode: 948, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.416028, mean_q: 2.503040, mean_eps: 0.000000\n",
      " 4367/5000: episode: 949, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.535466, mean_q: 2.657178, mean_eps: 0.000000\n",
      " 4371/5000: episode: 950, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.401272, mean_q: 2.487554, mean_eps: 0.000000\n",
      " 4375/5000: episode: 951, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000001, mean_squared_error: 1.482607, mean_q: 2.603678, mean_eps: 0.000000\n",
      " 4379/5000: episode: 952, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.297837, mean_q: 2.348440, mean_eps: 0.000000\n",
      " 4383/5000: episode: 953, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.448781, mean_q: 2.517752, mean_eps: 0.000000\n",
      " 4387/5000: episode: 954, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.428411, mean_q: 2.518384, mean_eps: 0.000000\n",
      " 4391/5000: episode: 955, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.496252, mean_q: 2.602815, mean_eps: 0.000000\n",
      " 4395/5000: episode: 956, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.323477, mean_q: 2.387232, mean_eps: 0.000000\n",
      " 4399/5000: episode: 957, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.384431, mean_q: 2.480164, mean_eps: 0.000000\n",
      " 4403/5000: episode: 958, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.412233, mean_q: 2.502739, mean_eps: 0.000000\n",
      " 4407/5000: episode: 959, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.322128, mean_q: 2.395240, mean_eps: 0.000000\n",
      " 4411/5000: episode: 960, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.422447, mean_q: 2.518453, mean_eps: 0.000000\n",
      " 4415/5000: episode: 961, duration: 0.053s, episode steps: 4, steps per second: 75, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.270259, mean_q: 2.325756, mean_eps: 0.000000\n",
      " 4419/5000: episode: 962, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.471849, mean_q: 2.564241, mean_eps: 0.000000\n",
      " 4423/5000: episode: 963, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.265331, mean_q: 2.325843, mean_eps: 0.000000\n",
      " 4427/5000: episode: 964, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.329243, mean_q: 2.410843, mean_eps: 0.000000\n",
      " 4431/5000: episode: 965, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.388666, mean_q: 2.464263, mean_eps: 0.000000\n",
      " 4435/5000: episode: 966, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.321695, mean_q: 2.387220, mean_eps: 0.000000\n",
      " 4439/5000: episode: 967, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.331351, mean_q: 2.402666, mean_eps: 0.000000\n",
      " 4443/5000: episode: 968, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.340553, mean_q: 2.418361, mean_eps: 0.000000\n",
      " 4447/5000: episode: 969, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.469643, mean_q: 2.603887, mean_eps: 0.000000\n",
      " 4451/5000: episode: 970, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.395353, mean_q: 2.479595, mean_eps: 0.000000\n",
      " 4455/5000: episode: 971, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.480257, mean_q: 2.595535, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4459/5000: episode: 972, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.323793, mean_q: 2.394903, mean_eps: 0.000000\n",
      " 4463/5000: episode: 973, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.447157, mean_q: 2.549074, mean_eps: 0.000000\n",
      " 4467/5000: episode: 974, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.368091, mean_q: 2.472347, mean_eps: 0.000000\n",
      " 4471/5000: episode: 975, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.283201, mean_q: 2.332995, mean_eps: 0.000000\n",
      " 4475/5000: episode: 976, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.451407, mean_q: 2.548971, mean_eps: 0.000000\n",
      " 4479/5000: episode: 977, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.403966, mean_q: 2.495067, mean_eps: 0.000000\n",
      " 4483/5000: episode: 978, duration: 0.062s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.380649, mean_q: 2.464272, mean_eps: 0.000000\n",
      " 4487/5000: episode: 979, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.496470, mean_q: 2.610579, mean_eps: 0.000000\n",
      " 4491/5000: episode: 980, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.350160, mean_q: 2.433669, mean_eps: 0.000000\n",
      " 4495/5000: episode: 981, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.365929, mean_q: 2.456617, mean_eps: 0.000000\n",
      " 4499/5000: episode: 982, duration: 0.067s, episode steps: 4, steps per second: 60, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.363253, mean_q: 2.417149, mean_eps: 0.000000\n",
      " 4503/5000: episode: 983, duration: 0.066s, episode steps: 4, steps per second: 60, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.378160, mean_q: 2.456394, mean_eps: 0.000000\n",
      " 4507/5000: episode: 984, duration: 0.066s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.436384, mean_q: 2.517525, mean_eps: 0.000000\n",
      " 4511/5000: episode: 985, duration: 0.067s, episode steps: 4, steps per second: 59, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.368956, mean_q: 2.448642, mean_eps: 0.000000\n",
      " 4515/5000: episode: 986, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.452425, mean_q: 2.540828, mean_eps: 0.000000\n",
      " 4519/5000: episode: 987, duration: 0.066s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.377444, mean_q: 2.456286, mean_eps: 0.000000\n",
      " 4523/5000: episode: 988, duration: 0.065s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.467519, mean_q: 2.564126, mean_eps: 0.000000\n",
      " 4527/5000: episode: 989, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.442519, mean_q: 2.533334, mean_eps: 0.000000\n",
      " 4531/5000: episode: 990, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.353453, mean_q: 2.425292, mean_eps: 0.000000\n",
      " 4535/5000: episode: 991, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.298702, mean_q: 2.363988, mean_eps: 0.000000\n",
      " 4539/5000: episode: 992, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.436489, mean_q: 2.533534, mean_eps: 0.000000\n",
      " 4543/5000: episode: 993, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.521480, mean_q: 2.641521, mean_eps: 0.000000\n",
      " 4547/5000: episode: 994, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.364100, mean_q: 2.449068, mean_eps: 0.000000\n",
      " 4551/5000: episode: 995, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.368035, mean_q: 2.448713, mean_eps: 0.000000\n",
      " 4555/5000: episode: 996, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.439333, mean_q: 2.541317, mean_eps: 0.000000\n",
      " 4559/5000: episode: 997, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.357628, mean_q: 2.441201, mean_eps: 0.000000\n",
      " 4563/5000: episode: 998, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.332853, mean_q: 2.402521, mean_eps: 0.000000\n",
      " 4567/5000: episode: 999, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.334137, mean_q: 2.394568, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4571/5000: episode: 1000, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.439602, mean_q: 2.533466, mean_eps: 0.000000\n",
      " 4575/5000: episode: 1001, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.324321, mean_q: 2.386875, mean_eps: 0.000000\n",
      " 4579/5000: episode: 1002, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.294413, mean_q: 2.364105, mean_eps: 0.000000\n",
      " 4583/5000: episode: 1003, duration: 0.064s, episode steps: 4, steps per second: 62, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.393229, mean_q: 2.463689, mean_eps: 0.000000\n",
      " 4587/5000: episode: 1004, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.417511, mean_q: 2.518447, mean_eps: 0.000000\n",
      " 4591/5000: episode: 1005, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.456568, mean_q: 2.580493, mean_eps: 0.000000\n",
      " 4595/5000: episode: 1006, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.364752, mean_q: 2.441134, mean_eps: 0.000000\n",
      " 4599/5000: episode: 1007, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.330874, mean_q: 2.378900, mean_eps: 0.000000\n",
      " 4603/5000: episode: 1008, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.469917, mean_q: 2.564131, mean_eps: 0.000000\n",
      " 4607/5000: episode: 1009, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.290704, mean_q: 2.340688, mean_eps: 0.000000\n",
      " 4611/5000: episode: 1010, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.279566, mean_q: 2.348894, mean_eps: 0.000000\n",
      " 4615/5000: episode: 1011, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.421422, mean_q: 2.502468, mean_eps: 0.000000\n",
      " 4619/5000: episode: 1012, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.422212, mean_q: 2.510258, mean_eps: 0.000000\n",
      " 4623/5000: episode: 1013, duration: 0.058s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.410800, mean_q: 2.494823, mean_eps: 0.000000\n",
      " 4627/5000: episode: 1014, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.405485, mean_q: 2.471306, mean_eps: 0.000000\n",
      " 4631/5000: episode: 1015, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.377269, mean_q: 2.440732, mean_eps: 0.000000\n",
      " 4635/5000: episode: 1016, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.385058, mean_q: 2.448235, mean_eps: 0.000000\n",
      " 4639/5000: episode: 1017, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.369485, mean_q: 2.440727, mean_eps: 0.000000\n",
      " 4643/5000: episode: 1018, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.324579, mean_q: 2.379059, mean_eps: 0.000000\n",
      " 4647/5000: episode: 1019, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.498775, mean_q: 2.602916, mean_eps: 0.000000\n",
      " 4651/5000: episode: 1020, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.484553, mean_q: 2.587479, mean_eps: 0.000000\n",
      " 4655/5000: episode: 1021, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.399297, mean_q: 2.487351, mean_eps: 0.000000\n",
      " 4659/5000: episode: 1022, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.401470, mean_q: 2.471469, mean_eps: 0.000000\n",
      " 4663/5000: episode: 1023, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.483230, mean_q: 2.595406, mean_eps: 0.000000\n",
      " 4667/5000: episode: 1024, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.312574, mean_q: 2.363879, mean_eps: 0.000000\n",
      " 4671/5000: episode: 1025, duration: 0.067s, episode steps: 4, steps per second: 60, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.325098, mean_q: 2.394975, mean_eps: 0.000000\n",
      " 4675/5000: episode: 1026, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.326329, mean_q: 2.379111, mean_eps: 0.000000\n",
      " 4679/5000: episode: 1027, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.353666, mean_q: 2.425541, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4683/5000: episode: 1028, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.475843, mean_q: 2.587784, mean_eps: 0.000000\n",
      " 4687/5000: episode: 1029, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.387213, mean_q: 2.456124, mean_eps: 0.000000\n",
      " 4691/5000: episode: 1030, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.482493, mean_q: 2.579541, mean_eps: 0.000000\n",
      " 4695/5000: episode: 1031, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.339855, mean_q: 2.410171, mean_eps: 0.000000\n",
      " 4699/5000: episode: 1032, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.499015, mean_q: 2.618646, mean_eps: 0.000000\n",
      " 4703/5000: episode: 1033, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.190793, mean_q: 2.217383, mean_eps: 0.000000\n",
      " 4707/5000: episode: 1034, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.410715, mean_q: 2.502807, mean_eps: 0.000000\n",
      " 4711/5000: episode: 1035, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.350071, mean_q: 2.433701, mean_eps: 0.000000\n",
      " 4715/5000: episode: 1036, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.404346, mean_q: 2.495243, mean_eps: 0.000000\n",
      " 4719/5000: episode: 1037, duration: 0.062s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.530366, mean_q: 2.672815, mean_eps: 0.000000\n",
      " 4723/5000: episode: 1038, duration: 0.058s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.338809, mean_q: 2.394392, mean_eps: 0.000000\n",
      " 4727/5000: episode: 1039, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.555557, mean_q: 2.680118, mean_eps: 0.000000\n",
      " 4731/5000: episode: 1040, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.338562, mean_q: 2.394390, mean_eps: 0.000000\n",
      " 4735/5000: episode: 1041, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.285561, mean_q: 2.364605, mean_eps: 0.000000\n",
      " 4739/5000: episode: 1042, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.220943, mean_q: 2.256299, mean_eps: 0.000000\n",
      " 4743/5000: episode: 1043, duration: 0.061s, episode steps: 4, steps per second: 65, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.563636, mean_q: 2.679671, mean_eps: 0.000000\n",
      " 4747/5000: episode: 1044, duration: 0.076s, episode steps: 4, steps per second: 52, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.421169, mean_q: 2.502609, mean_eps: 0.000000\n",
      " 4751/5000: episode: 1045, duration: 0.060s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.448627, mean_q: 2.541115, mean_eps: 0.000000\n",
      " 4755/5000: episode: 1046, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.424391, mean_q: 2.526038, mean_eps: 0.000000\n",
      " 4759/5000: episode: 1047, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.351080, mean_q: 2.433644, mean_eps: 0.000000\n",
      " 4763/5000: episode: 1048, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.393087, mean_q: 2.463705, mean_eps: 0.000000\n",
      " 4767/5000: episode: 1049, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.514037, mean_q: 2.634029, mean_eps: 0.000000\n",
      " 4771/5000: episode: 1050, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.380161, mean_q: 2.456535, mean_eps: 0.000000\n",
      " 4775/5000: episode: 1051, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.346209, mean_q: 2.433638, mean_eps: 0.000000\n",
      " 4779/5000: episode: 1052, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.408463, mean_q: 2.502886, mean_eps: 0.000000\n",
      " 4783/5000: episode: 1053, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.381377, mean_q: 2.464146, mean_eps: 0.000000\n",
      " 4787/5000: episode: 1054, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.500364, mean_q: 2.602803, mean_eps: 0.000000\n",
      " 4791/5000: episode: 1055, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.267422, mean_q: 2.317814, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4795/5000: episode: 1056, duration: 0.065s, episode steps: 4, steps per second: 61, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.296523, mean_q: 2.364343, mean_eps: 0.000000\n",
      " 4799/5000: episode: 1057, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.501263, mean_q: 2.618450, mean_eps: 0.000000\n",
      " 4803/5000: episode: 1058, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.406387, mean_q: 2.502966, mean_eps: 0.000000\n",
      " 4807/5000: episode: 1059, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.464070, mean_q: 2.572118, mean_eps: 0.000000\n",
      " 4811/5000: episode: 1060, duration: 0.063s, episode steps: 4, steps per second: 64, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.351011, mean_q: 2.441594, mean_eps: 0.000000\n",
      " 4815/5000: episode: 1061, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.354180, mean_q: 2.433586, mean_eps: 0.000000\n",
      " 4819/5000: episode: 1062, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.390528, mean_q: 2.471799, mean_eps: 0.000000\n",
      " 4823/5000: episode: 1063, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.280608, mean_q: 2.325169, mean_eps: 0.000000\n",
      " 4827/5000: episode: 1064, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.401912, mean_q: 2.479463, mean_eps: 0.000000\n",
      " 4831/5000: episode: 1065, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.348528, mean_q: 2.425954, mean_eps: 0.000000\n",
      " 4835/5000: episode: 1066, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.368192, mean_q: 2.448952, mean_eps: 0.000000\n",
      " 4839/5000: episode: 1067, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.214431, mean_q: 2.272097, mean_eps: 0.000000\n",
      " 4843/5000: episode: 1068, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.369893, mean_q: 2.456736, mean_eps: 0.000000\n",
      " 4847/5000: episode: 1069, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.462482, mean_q: 2.588073, mean_eps: 0.000000\n",
      " 4851/5000: episode: 1070, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.530695, mean_q: 2.649269, mean_eps: 0.000000\n",
      " 4855/5000: episode: 1071, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.361477, mean_q: 2.433384, mean_eps: 0.000000\n",
      " 4859/5000: episode: 1072, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.494647, mean_q: 2.610892, mean_eps: 0.000000\n",
      " 4863/5000: episode: 1073, duration: 0.056s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.341050, mean_q: 2.394340, mean_eps: 0.000000\n",
      " 4867/5000: episode: 1074, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.356704, mean_q: 2.433601, mean_eps: 0.000000\n",
      " 4871/5000: episode: 1075, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.462216, mean_q: 2.580304, mean_eps: 0.000000\n",
      " 4875/5000: episode: 1076, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.379681, mean_q: 2.440661, mean_eps: 0.000000\n",
      " 4879/5000: episode: 1077, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.325516, mean_q: 2.395084, mean_eps: 0.000000\n",
      " 4883/5000: episode: 1078, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.519692, mean_q: 2.657646, mean_eps: 0.000000\n",
      " 4887/5000: episode: 1079, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.191284, mean_q: 2.217578, mean_eps: 0.000000\n",
      " 4891/5000: episode: 1080, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.444435, mean_q: 2.549376, mean_eps: 0.000000\n",
      " 4895/5000: episode: 1081, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.331990, mean_q: 2.410911, mean_eps: 0.000000\n",
      " 4899/5000: episode: 1082, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.313326, mean_q: 2.387504, mean_eps: 0.000000\n",
      " 4903/5000: episode: 1083, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.420601, mean_q: 2.510651, mean_eps: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4907/5000: episode: 1084, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.380792, mean_q: 2.464480, mean_eps: 0.000000\n",
      " 4911/5000: episode: 1085, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.450802, mean_q: 2.565208, mean_eps: 0.000000\n",
      " 4915/5000: episode: 1086, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.370474, mean_q: 2.441032, mean_eps: 0.000000\n",
      " 4919/5000: episode: 1087, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.332086, mean_q: 2.387185, mean_eps: 0.000000\n",
      " 4923/5000: episode: 1088, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.367648, mean_q: 2.433327, mean_eps: 0.000000\n",
      " 4927/5000: episode: 1089, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.429486, mean_q: 2.518138, mean_eps: 0.000000\n",
      " 4931/5000: episode: 1090, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.385668, mean_q: 2.472026, mean_eps: 0.000000\n",
      " 4935/5000: episode: 1091, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.403441, mean_q: 2.503199, mean_eps: 0.000000\n",
      " 4939/5000: episode: 1092, duration: 0.055s, episode steps: 4, steps per second: 73, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.522986, mean_q: 2.633764, mean_eps: 0.000000\n",
      " 4943/5000: episode: 1093, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.299780, mean_q: 2.340551, mean_eps: 0.000000\n",
      " 4947/5000: episode: 1094, duration: 0.061s, episode steps: 4, steps per second: 66, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.444510, mean_q: 2.549372, mean_eps: 0.000000\n",
      " 4951/5000: episode: 1095, duration: 0.057s, episode steps: 4, steps per second: 71, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.488967, mean_q: 2.603087, mean_eps: 0.000000\n",
      " 4955/5000: episode: 1096, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.397089, mean_q: 2.471839, mean_eps: 0.000000\n",
      " 4959/5000: episode: 1097, duration: 0.059s, episode steps: 4, steps per second: 68, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.388527, mean_q: 2.456168, mean_eps: 0.000000\n",
      " 4963/5000: episode: 1098, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.417417, mean_q: 2.502449, mean_eps: 0.000000\n",
      " 4967/5000: episode: 1099, duration: 0.054s, episode steps: 4, steps per second: 74, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.453712, mean_q: 2.572666, mean_eps: 0.000000\n",
      " 4971/5000: episode: 1100, duration: 0.059s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.349912, mean_q: 2.417947, mean_eps: 0.000000\n",
      " 4975/5000: episode: 1101, duration: 0.060s, episode steps: 4, steps per second: 67, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.381354, mean_q: 2.464483, mean_eps: 0.000000\n",
      " 4979/5000: episode: 1102, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.466782, mean_q: 2.572230, mean_eps: 0.000000\n",
      " 4983/5000: episode: 1103, duration: 0.063s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.321461, mean_q: 2.410939, mean_eps: 0.000000\n",
      " 4987/5000: episode: 1104, duration: 0.064s, episode steps: 4, steps per second: 63, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.392583, mean_q: 2.464039, mean_eps: 0.000000\n",
      " 4991/5000: episode: 1105, duration: 0.056s, episode steps: 4, steps per second: 72, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.333087, mean_q: 2.378914, mean_eps: 0.000000\n",
      " 4995/5000: episode: 1106, duration: 0.057s, episode steps: 4, steps per second: 70, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.483844, mean_q: 2.587604, mean_eps: 0.000000\n",
      " 4999/5000: episode: 1107, duration: 0.058s, episode steps: 4, steps per second: 69, episode reward: 4.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.139 [0.000, 1.000], loss: 0.000000, mean_squared_error: 1.491526, mean_q: 2.603097, mean_eps: 0.000000\n",
      "done, took 69.626 seconds\n"
     ]
    }
   ],
   "source": [
    "# train the DQN\n",
    "env.reset()\n",
    "hist = dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71a5f551d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['episode_reward'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, env, nb_episodes=1, action_repetition=1, callbacks=None, visualize=True,\n",
    "         nb_max_episode_steps=None, nb_max_start_steps=0, start_step_policy=None, verbose=1):\n",
    "    if not self.compiled:\n",
    "        raise RuntimeError('Your tried to test your agent but it hasn\\'t been compiled yet. Please call `compile()` before `test()`.')\n",
    "    if action_repetition < 1:\n",
    "        raise ValueError('action_repetition must be >= 1, is {}'.format(action_repetition))\n",
    "\n",
    "    self.training = False\n",
    "    self.step = 0\n",
    "\n",
    "    callbacks = [] if not callbacks else callbacks[:]\n",
    "\n",
    "    if verbose >= 1:\n",
    "        callbacks += [TestLogger()]\n",
    "    if visualize:\n",
    "        callbacks += [Visualizer()]\n",
    "    history = History()\n",
    "    callbacks += [history]\n",
    "    callbacks = CallbackList(callbacks)\n",
    "    if hasattr(callbacks, 'set_model'):\n",
    "        callbacks.set_model(self)\n",
    "    else:\n",
    "        callbacks._set_model(self)\n",
    "    callbacks._set_env(env)\n",
    "    params = {\n",
    "        'nb_episodes': nb_episodes,\n",
    "    }\n",
    "    if hasattr(callbacks, 'set_params'):\n",
    "        callbacks.set_params(params)\n",
    "    else:\n",
    "        callbacks._set_params(params)\n",
    "\n",
    "    self._on_test_begin()\n",
    "    callbacks.on_train_begin()\n",
    "    for episode in range(nb_episodes):\n",
    "        callbacks.on_episode_begin(episode)\n",
    "        episode_reward = 0.\n",
    "        episode_step = 0\n",
    "\n",
    "        # Obtain the initial observation by resetting the environment.\n",
    "        self.reset_states()\n",
    "        observation = deepcopy(env.reset())\n",
    "        if self.processor is not None:\n",
    "            observation = self.processor.process_observation(observation)\n",
    "        assert observation is not None\n",
    "\n",
    "        # Perform random starts at beginning of episode and do not record them into the experience.\n",
    "        # This slightly changes the start position between games.\n",
    "        nb_random_start_steps = 0 if nb_max_start_steps == 0 else np.random.randint(nb_max_start_steps)\n",
    "        for _ in range(nb_random_start_steps):\n",
    "            if start_step_policy is None:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = start_step_policy(observation)\n",
    "            if self.processor is not None:\n",
    "                action = self.processor.process_action(action)\n",
    "            callbacks.on_action_begin(action)\n",
    "            observation, r, done, info = env.step(action)\n",
    "            observation = deepcopy(observation)\n",
    "            if self.processor is not None:\n",
    "                observation, r, done, info = self.processor.process_step(observation, r, done, info)\n",
    "            callbacks.on_action_end(action)\n",
    "            if done:\n",
    "                warnings.warn('Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.'.format(nb_random_start_steps))\n",
    "                observation = deepcopy(env.reset())\n",
    "                if self.processor is not None:\n",
    "                    observation = self.processor.process_observation(observation)\n",
    "                break\n",
    "\n",
    "        # Run the episode until we're done.\n",
    "        done = False\n",
    "        while not done:\n",
    "            callbacks.on_step_begin(episode_step)\n",
    "\n",
    "            action = self.forward(observation)\n",
    "            q_values = self.compute_q_values(np.array([observation]))\n",
    "            print(\"Q:\", q_values)\n",
    "            if self.processor is not None:\n",
    "                action = self.processor.process_action(action)\n",
    "            reward = 0.\n",
    "            accumulated_info = {}\n",
    "            for _ in range(action_repetition):\n",
    "                callbacks.on_action_begin(action)\n",
    "                observation, r, d, info = env.step(action)\n",
    "                observation = deepcopy(observation)\n",
    "                if self.processor is not None:\n",
    "                    observation, r, d, info = self.processor.process_step(observation, r, d, info)\n",
    "                callbacks.on_action_end(action)\n",
    "                reward += r\n",
    "                for key, value in info.items():\n",
    "                    if not np.isreal(value):\n",
    "                        continue\n",
    "                    if key not in accumulated_info:\n",
    "                        accumulated_info[key] = np.zeros_like(value)\n",
    "                    accumulated_info[key] += value\n",
    "                if d:\n",
    "                    done = True\n",
    "                    break\n",
    "            if nb_max_episode_steps and episode_step >= nb_max_episode_steps - 1:\n",
    "                done = True\n",
    "            self.backward(reward, terminal=done)\n",
    "            episode_reward += reward\n",
    "\n",
    "            step_logs = {\n",
    "                'action': action,\n",
    "                'observation': observation,\n",
    "                'reward': reward,\n",
    "                'episode': episode,\n",
    "                'info': accumulated_info,\n",
    "            }\n",
    "            callbacks.on_step_end(episode_step, step_logs)\n",
    "            episode_step += 1\n",
    "            self.step += 1\n",
    "\n",
    "        # We are in a terminal state but the agent hasn't yet seen it. We therefore\n",
    "        # perform one more forward-backward call and simply ignore the action before\n",
    "        # resetting the environment. We need to pass in `terminal=False` here since\n",
    "        # the *next* state, that is the state of the newly reset environment, is\n",
    "        # always non-terminal by convention.\n",
    "        self.forward(observation)\n",
    "        self.backward(0., terminal=False)\n",
    "\n",
    "        # Report end of episode.\n",
    "        episode_logs = {\n",
    "            'episode_reward': episode_reward,\n",
    "            'nb_steps': episode_step,\n",
    "        }\n",
    "        callbacks.on_episode_end(episode, episode_logs)\n",
    "    callbacks.on_train_end()\n",
    "    self._on_test_end()\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Q: [0.8051367  3.9638166  0.66204226 2.9440558 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71a581a390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: [0.26076916 2.7462535  0.45243543 2.9665895 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71a630c7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: [0.16215079 1.9870085  0.24613228 1.9883579 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71a2ac4160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: [-0.09244825  0.99999774  0.1287565   0.8802551 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71a2798c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'episode_reward': 4.0, 'nb_steps': 4}\n",
      "Episode 1: reward: 4.000, steps: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f71a581a0b8>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the DQN\n",
    "env.reset()\n",
    "test(dqn, env, nb_episodes=1, callbacks=[BasicCallback()], visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
